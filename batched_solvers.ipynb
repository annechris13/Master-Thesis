{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "batched solvers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3GFF6VDLHHaxcltKyIM02",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annechris13/Master-Thesis/blob/master/batched_solvers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCiWGq_Laseo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from enum import Enum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKHPRUoRa6HT",
        "colab_type": "code",
        "outputId": "cbfc7464-d268-4873-c5f0-bbd1609ccb88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"Solve a batch of QPs.\n",
        "            This function solves a batch of QPs, each optimizing over\n",
        "            `nz` variables and having `nineq` inequality constraints\n",
        "            and `neq` equality constraints.\n",
        "            The optimization problem for each instance in the batch\n",
        "            (dropping indexing from the notation) is of the form\n",
        "                \\hat z =   argmin_z 1/2 z^T Q z + p^T z\n",
        "                        subject to Gz <= h\n",
        "                                    Az  = b\n",
        "            where Q \\in S^{nz,nz},\n",
        "                S^{nz,nz} is the set of all positive semi-definite matrices,\n",
        "                p \\in R^{nz}\n",
        "                G \\in R^{nineq,nz}\n",
        "                h \\in R^{nineq}\n",
        "                A \\in R^{neq,nz}\n",
        "                b \\in R^{neq}\n",
        "            These parameters should all be passed to this function as\n",
        "            Variable- or Parameter-wrapped Tensors.\n",
        "            (See torch.autograd.Variable and torch.nn.parameter.Parameter)\n",
        "            If you want to solve a batch of QPs where `nz`, `nineq` and `neq`\n",
        "            are the same, but some of the contents differ across the\n",
        "            minibatch, you can pass in tensors in the standard way\n",
        "            where the first dimension indicates the batch example.\n",
        "            This can be done with some or all of the coefficients.\n",
        "            You do not need to add an extra dimension to coefficients\n",
        "            that will not change across all of the minibatch examples.\n",
        "            This function is able to infer such cases.\n",
        "            If you don't want to use any equality or inequality constraints,\n",
        "            you can set the appropriate values to:\n",
        "                e = Variable(torch.Tensor())\n",
        "            Parameters:\n",
        "            Q:  A (nBatch, nz, nz) or (nz, nz) Tensor.\n",
        "            p:  A (nBatch, nz) or (nz) Tensor.\n",
        "            G:  A (nBatch, nineq, nz) or (nineq, nz) Tensor.\n",
        "            h:  A (nBatch, nineq) or (nineq) Tensor.\n",
        "            A:  A (nBatch, neq, nz) or (neq, nz) Tensor.\n",
        "            b:  A (nBatch, neq) or (neq) Tensor.\n",
        "            Returns: \\hat z: a (nBatch, nz) Tensor.\n",
        "            \"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Solve a batch of QPs.\\n            This function solves a batch of QPs, each optimizing over\\n            `nz` variables and having `nineq` inequality constraints\\n            and `neq` equality constraints.\\n            The optimization problem for each instance in the batch\\n            (dropping indexing from the notation) is of the form\\n                \\\\hat z =   argmin_z 1/2 z^T Q z + p^T z\\n                        subject to Gz <= h\\n                                    Az  = b\\n            where Q \\\\in S^{nz,nz},\\n                S^{nz,nz} is the set of all positive semi-definite matrices,\\n                p \\\\in R^{nz}\\n                G \\\\in R^{nineq,nz}\\n                h \\\\in R^{nineq}\\n                A \\\\in R^{neq,nz}\\n                b \\\\in R^{neq}\\n            These parameters should all be passed to this function as\\n            Variable- or Parameter-wrapped Tensors.\\n            (See torch.autograd.Variable and torch.nn.parameter.Parameter)\\n            If you want to solve a batch of QPs where `nz`, `nineq` and `neq`\\n            are the same, but some of the contents differ across the\\n            minibatch, you can pass in tensors in the standard way\\n            where the first dimension indicates the batch example.\\n            This can be done with some or all of the coefficients.\\n            You do not need to add an extra dimension to coefficients\\n            that will not change across all of the minibatch examples.\\n            This function is able to infer such cases.\\n            If you don't want to use any equality or inequality constraints,\\n            you can set the appropriate values to:\\n                e = Variable(torch.Tensor())\\n            Parameters:\\n            Q:  A (nBatch, nz, nz) or (nz, nz) Tensor.\\n            p:  A (nBatch, nz) or (nz) Tensor.\\n            G:  A (nBatch, nineq, nz) or (nineq, nz) Tensor.\\n            h:  A (nBatch, nineq) or (nineq) Tensor.\\n            A:  A (nBatch, neq, nz) or (neq, nz) Tensor.\\n            b:  A (nBatch, neq) or (neq) Tensor.\\n            Returns: \\\\hat z: a (nBatch, nz) Tensor.\\n            \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1TEKSzTkyO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbatch=2\n",
        "nBatch=nbatch\n",
        "nz=2\n",
        "nineq=2\n",
        "neq=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JOAm3wYbKgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q=torch.tensor([4,1,1,2]).view(nz,nz)\n",
        "p=torch.tensor([1,1]).view(nz)\n",
        "G=torch.tensor([-1,0,0,-1]).view(nineq,nz)\n",
        "h=torch.tensor([0,0]).view(nineq)\n",
        "A=torch.tensor([1,1]).view(neq,nz)\n",
        "b=torch.tensor([1]).view(neq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y2jkJCPKlxTl",
        "colab": {}
      },
      "source": [
        "#to do: extract dimensions from problem parameters + check/add batch dimension\n",
        "Q=torch.tensor([[4,1,1,2],[6,2,2,2]]).view(nbatch,nz,nz).type(torch.DoubleTensor)\n",
        "p=torch.tensor([[1,1],[1,6]]).view(nbatch,nz).type(torch.DoubleTensor)\n",
        "G=torch.tensor([[-1,0,0,-1],[-1,0,0,-1]]).view(nbatch,nineq,nz).type(torch.DoubleTensor)\n",
        "h=torch.tensor([[0,0],[0,0]]).view(nbatch,nineq).type(torch.DoubleTensor)\n",
        "A=torch.tensor([[1,1],[2,3]]).view(nbatch,neq,nz).type(torch.DoubleTensor)\n",
        "b=torch.tensor([[1],[4]]).view(nbatch,neq).type(torch.DoubleTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtSYuaxgeI5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check if Q is psd:\n",
        "for i in range(nbatch):\n",
        "  e,_=torch.eig(Q[i])\n",
        "  if not torch.all(e[:,0]>0):\n",
        "    raise RuntimeError(\"Q is not PSD\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHo8nWj2n_ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lu_hack(x):\n",
        "    data, pivots = x.lu(pivot=not x.is_cuda)\n",
        "    if x.is_cuda:\n",
        "        if x.ndimension() == 2:\n",
        "            pivots = torch.arange(1, 1+x.size(0)).int().cuda()\n",
        "        elif x.ndimension() == 3:\n",
        "            pivots = torch.arange(\n",
        "                1, 1+x.size(1),\n",
        "            ).unsqueeze(0).repeat(x.size(0), 1).int().cuda()\n",
        "        else:\n",
        "            assert False\n",
        "    return (data, pivots)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN-V9_oz3CDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KKTSolvers(Enum):\n",
        "    LU_FULL = 1\n",
        "    LU_PARTIAL = 2\n",
        "    IR_UNOPT = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6gx0gEp4eFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_step(v, dv):\n",
        "    a = -v / dv\n",
        "    a[dv > 0] = max(1.0, a.max())\n",
        "    return a.min(1)[0].squeeze()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcsvPo3PPs11",
        "colab_type": "code",
        "outputId": "9696e2af-7978-4fd3-fa63-7c637ae1f5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "rx=p\n",
        "rs=torch.zeros(nBatch, nineq).type_as(Q)\n",
        "rz=-h\n",
        "ry= -b if b is not None else None\n",
        "D = torch.eye(nineq).repeat(nBatch, 1, 1).type_as(Q)\n",
        "H_ = torch.zeros(nBatch, nz + nineq, nz + nineq).type_as(Q)\n",
        "H_[:, :nz, :nz] = Q\n",
        "H_[:, -nineq:, -nineq:] = D\n",
        "if neq > 0:\n",
        "  A_ = torch.cat([torch.cat([G, torch.eye(nineq).type_as(Q).repeat(nBatch, 1, 1)], 2),torch.cat([A, torch.zeros(nBatch, neq, nineq).type_as(Q)], 2)], 1)\n",
        "  g_ = torch.cat([rx, rs], 1)\n",
        "  h_ = torch.cat([rz, ry], 1)\n",
        "else:\n",
        "  A_ = torch.cat([G, torch.eye(nineq).type_as(Q)], 1)\n",
        "  g_ = torch.cat([rx, rs], 1)\n",
        "  h_ = rz\n",
        "H_  \n",
        "A_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.,  0.,  1.,  0.],\n",
              "         [ 0., -1.,  0.,  1.],\n",
              "         [ 1.,  1.,  0.,  0.]],\n",
              "\n",
              "        [[-1.,  0.,  1.,  0.],\n",
              "         [ 0., -1.,  0.,  1.],\n",
              "         [ 2.,  3.,  0.,  0.]]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dhtEEoQV_4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# H=torch.zeros(nbatch,nz+nineq+neq,nz+nineq+neq).type_as(Q)\n",
        "# H[:,:nz,:nz]=Q\n",
        "# H[:,:nz,nz:nz+nineq]=torch.transpose(G,2,1)\n",
        "# H[:,:nz,nz+nineq:nz+nineq+neq]=torch.transpose(A,2,1)\n",
        "# H[:,nz:nz+nineq,:nz]=G\n",
        "# H[:,nz:nz+nineq,nz:nz+nineq]=-1*torch.eye(nineq).type_as(Q)\n",
        "# H[:,nz:nz+nineq,nz+nineq:nz+nineq+neq]=torch.zeros(nbatch,nineq,neq).type_as(Q)\n",
        "# H[:,nz+nineq:nz+nineq+neq,:nz]=A\n",
        "# H[:,nz+nineq:nz+nineq+neq,nz:nz+nineq]=torch.zeros(nbatch,neq,nineq).type_as(Q)\n",
        "# H[:,nz+nineq:nz+nineq+neq,nz+nineq:nz+nineq+neq]=torch.zeros(nbatch,neq,neq).type_as(Q)\n",
        "# F=torch.cat((-p,h,b), dim=1).unsqueeze(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_yI2iQseG4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G_T=torch.transpose(G,dim0=2,dim1=1)\n",
        "A_T=torch.transpose(A,dim0=2,dim1=1)\n",
        "R1=torch.cat((Q,G_T,A_T), dim=2)\n",
        "R2=torch.cat((G,-1*torch.eye(nineq).type_as(Q).repeat(nBatch, 1, 1),torch.zeros(nbatch,nineq,neq).type_as(Q)), dim=2)\n",
        "R3=torch.cat((A,torch.zeros(nbatch,neq,nineq).type_as(Q),torch.zeros(nbatch,neq,neq).type_as(Q)), dim=2)\n",
        "H=torch.cat((R1,R2,R3),dim=1)\n",
        "F=torch.cat((-p,h,b), dim=1).unsqueeze(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1axCMQ2pkPq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G_T=torch.transpose(G,dim0=2,dim1=1)\n",
        "A_T=torch.transpose(A,dim0=2,dim1=1)\n",
        "R1=torch.cat((Q,torch.zeros(nbatch,nz,nineq).type_as(Q),G_T,A_T), dim=2)\n",
        "R2=torch.cat((torch.zeros(nbatch,nineq,nz).type_as(Q),D,torch.eye(nineq).type_as(Q).repeat(nbatch,1,1),torch.zeros(nbatch,nineq,neq).type_as(Q)),dim=2)\n",
        "R3=torch.cat((G,torch.eye(nineq).type_as(Q).repeat(nBatch, 1, 1),torch.zeros(nbatch,nineq,nineq+neq).type_as(Q)), dim=2)\n",
        "R4=torch.cat((A,torch.zeros(nbatch,neq,2*nineq+neq).type_as(Q)), dim=2)\n",
        "H=torch.cat((R1,R2,R3,R4),dim=1)\n",
        "F=torch.cat((-p,torch.zeros(nbatch,nineq).type_as(Q),h,b), dim=1).unsqueeze(2)\n",
        "#H"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neCAZ6faYX5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H_lu,H_piv= lu_hack(H)\n",
        "initial=F.lu_solve(H_lu,H_piv)\n",
        "# initial"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhJ2RLpfoFMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYU0aBNHhFhe",
        "colab_type": "code",
        "outputId": "36f93d53-b0e9-4deb-ad57-26fdbf92da08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "x=initial[:,:nz,:]\n",
        "z=initial[:,nz:nz+nineq,:]\n",
        "y=initial[:,nz+nineq:nz+nineq+neq,:]\n",
        "s=-z\n",
        "print(x,s,z,y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.3333],\n",
            "         [0.6667]],\n",
            "\n",
            "        [[0.5294],\n",
            "         [0.9804]]], dtype=torch.float64) tensor([[[0.3333],\n",
            "         [0.6667]],\n",
            "\n",
            "        [[0.5294],\n",
            "         [0.9804]]], dtype=torch.float64) tensor([[[-0.3333],\n",
            "         [-0.6667]],\n",
            "\n",
            "        [[-0.5294],\n",
            "         [-0.9804]]], dtype=torch.float64) tensor([[[-3.3333]],\n",
            "\n",
            "        [[-3.3333]]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDp68s0BigWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDGeaMOx4s11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def factor_solve_kkt(Q, D, G, A, rx, rs, rz, ry):\n",
        "    nineq, nz, neq, nBatch = get_sizes(G, A)\n",
        "\n",
        "    H_ = torch.zeros(nBatch, nz + nineq, nz + nineq).type_as(Q)\n",
        "    H_[:, :nz, :nz] = Q\n",
        "    H_[:, -nineq:, -nineq:] = D\n",
        "    if neq > 0:\n",
        "        A_ = torch.cat([torch.cat([G, torch.eye(nineq).type_as(Q).repeat(nBatch, 1, 1)], 2),\n",
        "                        torch.cat([A, torch.zeros(nBatch, neq, nineq).type_as(Q)], 2)], 1)\n",
        "        g_ = torch.cat([rx, rs], 1)\n",
        "        h_ = torch.cat([rz, ry], 1)\n",
        "    else:\n",
        "        A_ = torch.cat([G, torch.eye(nineq).type_as(Q)], 1)\n",
        "        g_ = torch.cat([rx, rs], 1)\n",
        "        h_ = rz\n",
        "\n",
        "    H_LU = lu_hack(H_)\n",
        "\n",
        "    invH_A_ = A_.transpose(1, 2).lu_solve(*H_LU)\n",
        "    invH_g_ = g_.unsqueeze(2).lu_solve(*H_LU).squeeze(2)\n",
        "\n",
        "    S_ = torch.bmm(A_, invH_A_)\n",
        "    S_LU = lu_hack(S_)\n",
        "    t_ = torch.bmm(invH_g_.unsqueeze(1), A_.transpose(1, 2)).squeeze(1) - h_\n",
        "    w_ = -t_.unsqueeze(2).lu_solve(*S_LU).squeeze(2)\n",
        "    t_ = -g_ - w_.unsqueeze(1).bmm(A_).squeeze()\n",
        "    v_ = t_.unsqueeze(2).lu_solve(*H_LU).squeeze(2)\n",
        "\n",
        "    dx = v_[:, :nz]\n",
        "    ds = v_[:, nz:]\n",
        "    dz = w_[:, :nineq]\n",
        "    dy = w_[:, nineq:] if neq > 0 else None\n",
        "\n",
        "    return dx, ds, dz, dy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NebhA8tJ5CWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sizes(G, A=None):\n",
        "    if G.dim() == 2:\n",
        "        nineq, nz = G.size()\n",
        "        nBatch = 1\n",
        "    elif G.dim() == 3:\n",
        "        nBatch, nineq, nz = G.size()\n",
        "    if A is not None:\n",
        "        neq = A.size(1) if A.nelement() > 0 else 0\n",
        "    else:\n",
        "        neq = None\n",
        "    # nBatch = batchedTensor.size(0) if batchedTensor is not None else None\n",
        "    return nineq, nz, neq, nBatch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGoTMS4A5PDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bdiag(d):\n",
        "    nBatch, sz = d.size()\n",
        "    D = torch.zeros(nBatch, sz, sz).type_as(d)\n",
        "    I = torch.eye(sz).repeat(nBatch, 1, 1).type_as(d).bool()\n",
        "    D[I] = d.squeeze().view(-1)\n",
        "    return D\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqnUL-IzMQ-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG_zAPPJ2tgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(Q, p, G, h, A, b, eps=1e-12, verbose=1, notImprovedLim=3,\n",
        "            maxIter=5, solver=KKTSolvers.LU_FULL):\n",
        "\n",
        "    nineq, nz, neq, nBatch = get_sizes(G, A)\n",
        "\n",
        "    # Find initial values\n",
        "    if solver == KKTSolvers.LU_FULL:\n",
        "        D = torch.eye(nineq).repeat(nBatch, 1, 1).type_as(Q)\n",
        "        print(D)\n",
        "        x, s, z, y = factor_solve_kkt(\n",
        "            Q, D, G, A, p,\n",
        "            torch.zeros(nBatch, nineq).type_as(Q),\n",
        "            -h, -b if b is not None else None)\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "  \n",
        "    best = {'resids': None, 'x': None, 'z': None, 's': None, 'y': None}\n",
        "    nNotImproved = 0\n",
        "    print(\"\\nInitialized as, x:{},s:{},y:{},z:{},\".format(\n",
        "        x,s,y,z\n",
        "    ))\n",
        "    for i in range(maxIter):\n",
        "        # affine scaling direction\n",
        "        rx = (torch.bmm(y.unsqueeze(1), A).squeeze(1) if neq > 0 else 0.) + \\\n",
        "            torch.bmm(z.unsqueeze(1), G).squeeze(1) + \\\n",
        "            torch.bmm(x.unsqueeze(1), Q.transpose(1, 2)).squeeze(1) + \\\n",
        "            p\n",
        "        rs = z\n",
        "        rz = torch.bmm(x.unsqueeze(1), G.transpose(1, 2)).squeeze(1) + s - h\n",
        "        ry = torch.bmm(x.unsqueeze(1), A.transpose(\n",
        "            1, 2)).squeeze(1) - b if neq > 0 else 0.0\n",
        "\n",
        "        mu = torch.abs((s * z).sum(1).squeeze() / nineq)\n",
        "        # print('\\nrx: {}, rz: {}, ry: {},'.format(\n",
        "        #     rx,rz,ry\n",
        "        # ))\n",
        "\n",
        "        z_resid = torch.norm(rz, 2, 1).squeeze()\n",
        "        y_resid = torch.norm(ry, 2, 1).squeeze() if neq > 0 else 0\n",
        "        pri_resid = y_resid + z_resid\n",
        "        dual_resid = torch.norm(rx, 2, 1).squeeze()\n",
        "        resids = pri_resid + dual_resid #+ nineq * mu\n",
        "\n",
        "        d = z / s\n",
        "        # try:\n",
        "        #     factor_kkt(S_LU, R, d)\n",
        "        # except:\n",
        "        #     return best['x'], best['y'], best['z'], best['s']\n",
        "\n",
        "        if verbose == 1:\n",
        "            print('iter: {}, pri_resid: {:.5e}, dual_resid: {:.5e}, mu: {:.5e}'.format(\n",
        "                i, pri_resid.mean(), dual_resid.mean(), mu.mean()))\n",
        "            print(x)\n",
        "        \n",
        "        if solver == KKTSolvers.LU_FULL:\n",
        "            D = bdiag(d)\n",
        "            dx_aff, ds_aff, dz_aff, dy_aff = factor_solve_kkt(\n",
        "                Q, D, G, A, rx, rs, rz, ry)\n",
        "    \n",
        "        else:\n",
        "            assert False\n",
        "        dx = dx_aff \n",
        "        ds = ds_aff \n",
        "        dz = dz_aff \n",
        "        dy = dy_aff \n",
        "\n",
        "        alpha = torch.min(0.999 * torch.min(get_step(z, dz),\n",
        "                                            get_step(s, ds)),\n",
        "                          torch.ones(nBatch).type_as(Q))\n",
        "        alpha_nineq = alpha.repeat(nineq, 1).t()\n",
        "        alpha_neq = alpha.repeat(neq, 1).t() if neq > 0 else None\n",
        "        alpha_nz = alpha.repeat(nz, 1).t()\n",
        "        \n",
        "        x += alpha_nz * dx\n",
        "        s += alpha_nineq * ds\n",
        "        z += alpha_nineq * dz\n",
        "        y = y + alpha_neq * dy if neq > 0 else None\n",
        "\n",
        "    return best['x'], best['y'], best['z'], best['s']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JvGur4z4aLj",
        "colab_type": "code",
        "outputId": "c4ffc996-d82c-44e8-d1a4-62ced109d74f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x,_,_,_=forward(Q,p,G,h,A,b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Initialized as, x:tensor([[0.3333, 0.6667],\n",
            "        [0.5294, 0.9804]], dtype=torch.float64),s:tensor([[0.3333, 0.6667],\n",
            "        [0.5294, 0.9804]], dtype=torch.float64),y:tensor([[-3.3333],\n",
            "        [-3.3333]], dtype=torch.float64),z:tensor([[-0.3333, -0.6667],\n",
            "        [-0.5294, -0.9804]], dtype=torch.float64),\n",
            "iter: 0, pri_resid: 4.01964e-16, dual_resid: 0.00000e+00, mu: 4.49250e-01\n",
            "tensor([[0.3333, 0.6667],\n",
            "        [0.5294, 0.9804]], dtype=torch.float64)\n",
            "iter: 1, pri_resid: 2.27170e-16, dual_resid: 6.92343e-16, mu: 2.98874e-02\n",
            "tensor([[0.1667, 0.8333],\n",
            "        [0.4847, 1.0102]], dtype=torch.float64)\n",
            "iter: 2, pri_resid: 1.24127e-16, dual_resid: 1.20220e-15, mu: 1.11071e-02\n",
            "tensor([[0.2708, 0.7292],\n",
            "        [0.5003, 0.9998]], dtype=torch.float64)\n",
            "iter: 3, pri_resid: 5.55112e-17, dual_resid: 6.66134e-16, mu: 6.22983e-04\n",
            "tensor([[0.2524, 0.7476],\n",
            "        [0.5000, 1.0000]], dtype=torch.float64)\n",
            "iter: 4, pri_resid: 1.17575e-16, dual_resid: 6.66134e-16, mu: 8.44852e-06\n",
            "tensor([[0.2500, 0.7500],\n",
            "        [0.5000, 1.0000]], dtype=torch.float64)\n",
            "iter: 5, pri_resid: 1.17575e-16, dual_resid: 1.11022e-15, mu: 9.96597e-09\n",
            "tensor([[0.2500, 0.7500],\n",
            "        [0.5000, 1.0000]], dtype=torch.float64)\n",
            "iter: 6, pri_resid: 1.86190e-16, dual_resid: 1.11022e-15, mu: 9.96809e-12\n",
            "tensor([[0.2500, 0.7500],\n",
            "        [0.5000, 1.0000]], dtype=torch.float64)\n",
            "iter: 7, pri_resid: nan, dual_resid: nan, mu: nan\n",
            "tensor([[0.2500, 0.7500],\n",
            "        [   inf,   -inf]], dtype=torch.float64)\n",
            "iter: 8, pri_resid: nan, dual_resid: nan, mu: nan\n",
            "tensor([[0.2500, 0.7500],\n",
            "        [   nan,    nan]], dtype=torch.float64)\n",
            "iter: 9, pri_resid: nan, dual_resid: nan, mu: nan\n",
            "tensor([[0.2500, 0.7500],\n",
            "        [   nan,    nan]], dtype=torch.float64)\n",
            "iter: 10, pri_resid: nan, dual_resid: nan, mu: nan\n",
            "tensor([[inf, -inf],\n",
            "        [nan, nan]], dtype=torch.float64)\n",
            "iter: 11, pri_resid: nan, dual_resid: nan, mu: nan\n",
            "tensor([[nan, nan],\n",
            "        [nan, nan]], dtype=torch.float64)\n",
            "iter: 12, pri_resid: nan, dual_resid: nan, mu: nan\n",
            "tensor([[nan, nan],\n",
            "        [nan, nan]], dtype=torch.float64)\n",
            "iter: 13, pri_resid: nan, dual_resid: nan, mu: nan\n",
            "tensor([[nan, nan],\n",
            "        [nan, nan]], dtype=torch.float64)\n",
            "iter: 14, pri_resid: nan, dual_resid: nan, mu: nan\n",
            "tensor([[nan, nan],\n",
            "        [nan, nan]], dtype=torch.float64)\n",
            "iter: 15, pri_resid: nan, dual_resid: nan, mu: nan\n",
            "tensor([[nan, nan],\n",
            "        [nan, nan]], dtype=torch.float64)\n",
            "iter: 16, pri_resid: nan, dual_resid: nan, mu: nan\n",
            "tensor([[nan, nan],\n",
            "        [nan, nan]], dtype=torch.float64)\n",
            "iter: 17, pri_resid: nan, dual_resid: nan, mu: nan\n",
            "tensor([[nan, nan],\n",
            "        [nan, nan]], dtype=torch.float64)\n",
            "iter: 18, pri_resid: nan, dual_resid: nan, mu: nan\n",
            "tensor([[nan, nan],\n",
            "        [nan, nan]], dtype=torch.float64)\n",
            "iter: 19, pri_resid: nan, dual_resid: nan, mu: nan\n",
            "tensor([[nan, nan],\n",
            "        [nan, nan]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-e4cdf2dacba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-115-ce09b1781800>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(Q, p, G, h, A, b, eps, verbose, notImprovedLim, maxIter, solver)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha_neq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneq\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'resids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINACC_ERR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'max'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uucvLFkw440G",
        "colab_type": "code",
        "outputId": "22131f71-29c4-4406-bed1-c3ad2f5f9282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x[0].T@Q[0]@x[0]\n",
        "x[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2500, 0.7500], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCwJTIvN59Ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in nbatch:\n",
        "  qpth_optimum=(0.5*np.sum((Q@x[i])**2)) + q.T @ qpth_solution"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}