{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/annechris13/Master-Thesis/blob/master/qpth_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rHRikTa9ua8c"
   },
   "outputs": [],
   "source": [
    "# !pip install qpth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GEzSnphYpz40"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from enum import Enum\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "import time\n",
    "# from qpth.qp import QPFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "drkLBoZ3qbLl"
   },
   "outputs": [],
   "source": [
    "def bdiag(d):\n",
    "    nBatch, sz = d.size()\n",
    "    D = torch.zeros(nBatch, sz, sz).type_as(d)\n",
    "    I = torch.eye(sz).repeat(nBatch, 1, 1).type_as(d).bool()\n",
    "    D[I] = d.squeeze().view(-1)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8Ks96d-qqkn2"
   },
   "outputs": [],
   "source": [
    "def get_sizes(G, A=None):\n",
    "    if G.dim() == 2:\n",
    "        nineq, nz = G.size()\n",
    "        nBatch = 1\n",
    "    elif G.dim() == 3:\n",
    "        nBatch, nineq, nz = G.size()\n",
    "    if A is not None:\n",
    "        neq = A.size(1) if A.nelement() > 0 else 0\n",
    "    else:\n",
    "        neq = None\n",
    "    # nBatch = batchedTensor.size(0) if batchedTensor is not None else None\n",
    "    return nineq, nz, neq, nBatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "v2K84NSFqnXG"
   },
   "outputs": [],
   "source": [
    "def lu_hack(x):\n",
    "    data, pivots = x.lu(pivot=not x.is_cuda)\n",
    "    if x.is_cuda:\n",
    "        if x.ndimension() == 2:\n",
    "            pivots = torch.arange(1, 1+x.size(0)).int().cuda()\n",
    "        elif x.ndimension() == 3:\n",
    "            pivots = torch.arange(\n",
    "                1, 1+x.size(1),\n",
    "            ).unsqueeze(0).repeat(x.size(0), 1).int().cuda()\n",
    "        else:\n",
    "            assert False\n",
    "    return (data, pivots)\n",
    "\n",
    "\n",
    "INACC_ERR = \"qpth warning: Returning an inaccurate and potentially incorrect solution.\"\n",
    "# Some residual is large.\n",
    "# Your problem may be infeasible or difficult.\n",
    "# You can try using the CVXPY solver to see if your problem is feasible\n",
    "# and you can use the verbose option to check the convergence status of\n",
    "# our solver while increasing the number of iterations.\n",
    "# Advanced users:\n",
    "# You can also try to enable iterative refinement in the solver:\n",
    "# https://github.com/locuslab/qpth/issues/6\n",
    "# --------\n",
    "\n",
    "\n",
    "\n",
    "class KKTSolvers(Enum):\n",
    "    LU_FULL = 1\n",
    "    LU_PARTIAL = 2\n",
    "    IR_UNOPT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "t9JXBe9Urmi1"
   },
   "outputs": [],
   "source": [
    "def forward(Q, p, G, h, A, b, Q_LU, S_LU, R, eps=1e-12, verbose=0, notImprovedLim=3,\n",
    "            maxIter=20, solver=KKTSolvers.LU_PARTIAL):\n",
    "    \"\"\"\n",
    "    Q_LU, S_LU, R = pre_factor_kkt(Q, G, A)\n",
    "    \"\"\"\n",
    "    nineq, nz, neq, nBatch = get_sizes(G, A)\n",
    "\n",
    "    # Find initial values\n",
    "    if solver == KKTSolvers.LU_FULL:\n",
    "        D = torch.eye(nineq).repeat(nBatch, 1, 1).type_as(Q)\n",
    "        x, s, z, y = factor_solve_kkt(\n",
    "            Q, D, G, A, p,\n",
    "            torch.zeros(nBatch, nineq).type_as(Q),\n",
    "            -h, -b if b is not None else None)\n",
    "    elif solver == KKTSolvers.LU_PARTIAL:\n",
    "        d = torch.ones(nBatch, nineq).type_as(Q)\n",
    "        factor_kkt(S_LU, R, d)\n",
    "        x, s, z, y = solve_kkt(\n",
    "            Q_LU, d, G, A, S_LU,\n",
    "            p, torch.zeros(nBatch, nineq).type_as(Q),\n",
    "            -h, -b if neq > 0 else None)\n",
    "    elif solver == KKTSolvers.IR_UNOPT:\n",
    "        D = torch.eye(nineq).repeat(nBatch, 1, 1).type_as(Q)\n",
    "        x, s, z, y = solve_kkt_ir(\n",
    "            Q, D, G, A, p,\n",
    "            torch.zeros(nBatch, nineq).type_as(Q),\n",
    "            -h, -b if b is not None else None)\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    # Make all of the slack variables >= 1.\n",
    "    M = torch.min(s, 1)[0]\n",
    "    M = M.view(M.size(0), 1).repeat(1, nineq)\n",
    "    I = M < 0\n",
    "    s[I] -= M[I] - 1\n",
    "\n",
    "    # Make all of the inequality dual variables >= 1.\n",
    "    M = torch.min(z, 1)[0]\n",
    "    M = M.view(M.size(0), 1).repeat(1, nineq)\n",
    "    I = M < 0\n",
    "    z[I] -= M[I] - 1\n",
    "\n",
    "    best = {'resids': None, 'x': None, 'z': None, 's': None, 'y': None}\n",
    "    nNotImproved = 0\n",
    "\n",
    "    for i in range(maxIter):\n",
    "        # affine scaling direction\n",
    "        rx = (torch.bmm(y.unsqueeze(1), A).squeeze(1) if neq > 0 else 0.) + \\\n",
    "            torch.bmm(z.unsqueeze(1), G).squeeze(1) + \\\n",
    "            torch.bmm(x.unsqueeze(1), Q.transpose(1, 2)).squeeze(1) + \\\n",
    "            p\n",
    "        rs = z\n",
    "        rz = torch.bmm(x.unsqueeze(1), G.transpose(1, 2)).squeeze(1) + s - h\n",
    "        ry = torch.bmm(x.unsqueeze(1), A.transpose(\n",
    "            1, 2)).squeeze(1) - b if neq > 0 else 0.0\n",
    "        mu = torch.abs((s * z).sum(1).squeeze() / nineq)\n",
    "        z_resid = torch.norm(rz, 2, 1).squeeze()\n",
    "        y_resid = torch.norm(ry, 2, 1).squeeze() if neq > 0 else 0\n",
    "        pri_resid = y_resid + z_resid\n",
    "        dual_resid = torch.norm(rx, 2, 1).squeeze()\n",
    "        resids = pri_resid + dual_resid + nineq * mu\n",
    "\n",
    "        d = z / s\n",
    "        try:\n",
    "            factor_kkt(S_LU, R, d)\n",
    "        except:\n",
    "            return best['x'], best['y'], best['z'], best['s']\n",
    "\n",
    "        if verbose == 1:\n",
    "            print('iter: {}, pri_resid: {:.5e}, dual_resid: {:.5e}, mu: {:.5e}'.format(\n",
    "                i, pri_resid.mean(), dual_resid.mean(), mu.mean()))\n",
    "        if best['resids'] is None:\n",
    "            best['resids'] = resids\n",
    "            best['x'] = x.clone()\n",
    "            best['z'] = z.clone()\n",
    "            best['s'] = s.clone()\n",
    "            best['y'] = y.clone() if y is not None else None\n",
    "            nNotImproved = 0\n",
    "        else:\n",
    "            I = resids < best['resids']\n",
    "            if I.sum() > 0:\n",
    "                nNotImproved = 0\n",
    "            else:\n",
    "                nNotImproved += 1\n",
    "            I_nz = I.repeat(nz, 1).t()\n",
    "            I_nineq = I.repeat(nineq, 1).t()\n",
    "            best['resids'][I] = resids[I]\n",
    "            best['x'][I_nz] = x[I_nz]\n",
    "            best['z'][I_nineq] = z[I_nineq]\n",
    "            best['s'][I_nineq] = s[I_nineq]\n",
    "            if neq > 0:\n",
    "                I_neq = I.repeat(neq, 1).t()\n",
    "                best['y'][I_neq] = y[I_neq]\n",
    "        if nNotImproved == notImprovedLim or best['resids'].max() < eps or mu.min() > 1e32:\n",
    "            if best['resids'].max() > 1. and verbose >= 0:\n",
    "                print(INACC_ERR)\n",
    "            return best['x'], best['y'], best['z'], best['s']\n",
    "\n",
    "        if solver == KKTSolvers.LU_FULL:\n",
    "            D = bdiag(d)\n",
    "            dx_aff, ds_aff, dz_aff, dy_aff = factor_solve_kkt(\n",
    "                Q, D, G, A, rx, rs, rz, ry)\n",
    "        elif solver == KKTSolvers.LU_PARTIAL:\n",
    "            dx_aff, ds_aff, dz_aff, dy_aff = solve_kkt(\n",
    "                Q_LU, d, G, A, S_LU, rx, rs, rz, ry)\n",
    "        elif solver == KKTSolvers.IR_UNOPT:\n",
    "            D = bdiag(d)\n",
    "            dx_aff, ds_aff, dz_aff, dy_aff = solve_kkt_ir(\n",
    "                Q, D, G, A, rx, rs, rz, ry)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        # compute centering directions\n",
    "        alpha = torch.min(torch.min(get_step(z, dz_aff),\n",
    "                                    get_step(s, ds_aff)),\n",
    "                          torch.ones(nBatch).type_as(Q))\n",
    "        alpha_nineq = alpha.repeat(nineq, 1).t()\n",
    "        t1 = s + alpha_nineq * ds_aff\n",
    "        t2 = z + alpha_nineq * dz_aff\n",
    "        t3 = torch.sum(t1 * t2, 1).squeeze()\n",
    "        t4 = torch.sum(s * z, 1).squeeze()\n",
    "        sig = (t3 / t4)**3\n",
    "\n",
    "        rx = torch.zeros(nBatch, nz).type_as(Q)\n",
    "        rs = ((-mu * sig).repeat(nineq, 1).t() + ds_aff * dz_aff) / s\n",
    "        rz = torch.zeros(nBatch, nineq).type_as(Q)\n",
    "        ry = torch.zeros(nBatch, neq).type_as(Q) if neq > 0 else torch.Tensor()\n",
    "\n",
    "        if solver == KKTSolvers.LU_FULL:\n",
    "            D = bdiag(d)\n",
    "            dx_cor, ds_cor, dz_cor, dy_cor = factor_solve_kkt(\n",
    "                Q, D, G, A, rx, rs, rz, ry)\n",
    "        elif solver == KKTSolvers.LU_PARTIAL:\n",
    "            dx_cor, ds_cor, dz_cor, dy_cor = solve_kkt(\n",
    "                Q_LU, d, G, A, S_LU, rx, rs, rz, ry)\n",
    "        elif solver == KKTSolvers.IR_UNOPT:\n",
    "            D = bdiag(d)\n",
    "            dx_cor, ds_cor, dz_cor, dy_cor = solve_kkt_ir(\n",
    "                Q, D, G, A, rx, rs, rz, ry)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        dx = dx_aff + dx_cor\n",
    "        ds = ds_aff + ds_cor\n",
    "        dz = dz_aff + dz_cor\n",
    "        dy = dy_aff + dy_cor if neq > 0 else None\n",
    "        alpha = torch.min(0.999 * torch.min(get_step(z, dz),\n",
    "                                            get_step(s, ds)),\n",
    "                          torch.ones(nBatch).type_as(Q))\n",
    "        alpha_nineq = alpha.repeat(nineq, 1).t()\n",
    "        alpha_neq = alpha.repeat(neq, 1).t() if neq > 0 else None\n",
    "        alpha_nz = alpha.repeat(nz, 1).t()\n",
    "\n",
    "        x += alpha_nz * dx\n",
    "        s += alpha_nineq * ds\n",
    "        z += alpha_nineq * dz\n",
    "        y = y + alpha_neq * dy if neq > 0 else None\n",
    "\n",
    "    if best['resids'].max() > 1. and verbose >= 0:\n",
    "        print(INACC_ERR)\n",
    "    return best['x'], best['y'], best['z'], best['s']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FqY6YR2Drqnf"
   },
   "outputs": [],
   "source": [
    "def get_step(v, dv):\n",
    "    a = -v / dv\n",
    "    a[dv > 0] = max(1.0, a.max())\n",
    "    return a.min(1)[0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3vDPDoX6rtEB"
   },
   "outputs": [],
   "source": [
    "def unpack_kkt(v, nz, nineq, neq):\n",
    "    i = 0\n",
    "    x = v[:, i:i + nz]\n",
    "    i += nz\n",
    "    s = v[:, i:i + nineq]\n",
    "    i += nineq\n",
    "    z = v[:, i:i + nineq]\n",
    "    i += nineq\n",
    "    y = v[:, i:i + neq]\n",
    "    return x, s, z, y\n",
    "\n",
    "\n",
    "def kkt_resid_reg(Q_tilde, D_tilde, G, A, eps, dx, ds, dz, dy, rx, rs, rz, ry):\n",
    "    dx, ds, dz, dy, rx, rs, rz, ry = [\n",
    "        x.unsqueeze(2) if x is not None else None for x in\n",
    "        [dx, ds, dz, dy, rx, rs, rz, ry]\n",
    "    ]\n",
    "    resx = Q_tilde.bmm(dx) + G.transpose(1, 2).bmm(dz) + rx\n",
    "    if dy is not None:\n",
    "        resx += A.transpose(1, 2).bmm(dy)\n",
    "    ress = D_tilde.bmm(ds) + dz + rs\n",
    "    resz = G.bmm(dx) + ds - eps * dz + rz\n",
    "    resy = A.bmm(dx) - eps * dy + ry if dy is not None else None\n",
    "    resx, ress, resz, resy = (\n",
    "        v.squeeze(2) if v is not None else None for v in (resx, ress, resz, resy))\n",
    "    return resx, ress, resz, resy\n",
    "\n",
    "\n",
    "def solve_kkt_ir(Q, D, G, A, rx, rs, rz, ry, niter=1):\n",
    "    \"\"\"Inefficient iterative refinement.\"\"\"\n",
    "    nineq, nz, neq, nBatch = get_sizes(G, A)\n",
    "\n",
    "    eps = 1e-7\n",
    "    Q_tilde = Q + eps * torch.eye(nz).type_as(Q).repeat(nBatch, 1, 1)\n",
    "    D_tilde = D + eps * torch.eye(nineq).type_as(Q).repeat(nBatch, 1, 1)\n",
    "\n",
    "    dx, ds, dz, dy = factor_solve_kkt_reg(\n",
    "        Q_tilde, D_tilde, G, A, rx, rs, rz, ry, eps)\n",
    "    res = kkt_resid_reg(Q, D, G, A, eps,\n",
    "                        dx, ds, dz, dy, rx, rs, rz, ry)\n",
    "    resx, ress, resz, resy = res\n",
    "    res = resx\n",
    "    for k in range(niter):\n",
    "        ddx, dds, ddz, ddy = factor_solve_kkt_reg(Q_tilde, D_tilde, G, A, -resx, -ress, -resz,\n",
    "                                                  -resy if resy is not None else None,\n",
    "                                                  eps)\n",
    "        dx, ds, dz, dy = [v + dv if v is not None else None\n",
    "                          for v, dv in zip((dx, ds, dz, dy), (ddx, dds, ddz, ddy))]\n",
    "        res = kkt_resid_reg(Q, D, G, A, eps,\n",
    "                            dx, ds, dz, dy, rx, rs, rz, ry)\n",
    "        resx, ress, resz, resy = res\n",
    "        # res = torch.cat(resx)\n",
    "        res = resx\n",
    "\n",
    "    return dx, ds, dz, dy\n",
    "\n",
    "\n",
    "def factor_solve_kkt_reg(Q_tilde, D, G, A, rx, rs, rz, ry, eps):\n",
    "    nineq, nz, neq, nBatch = get_sizes(G, A)\n",
    "\n",
    "    H_ = torch.zeros(nBatch, nz + nineq, nz + nineq).type_as(Q_tilde)\n",
    "    H_[:, :nz, :nz] = Q_tilde\n",
    "    H_[:, -nineq:, -nineq:] = D\n",
    "    if neq > 0:\n",
    "        # H_ = torch.cat([torch.cat([Q, torch.zeros(nz,nineq).type_as(Q)], 1),\n",
    "        # torch.cat([torch.zeros(nineq, nz).type_as(Q), D], 1)], 0)\n",
    "        A_ = torch.cat([torch.cat([G, torch.eye(nineq).type_as(Q_tilde).repeat(nBatch, 1, 1)], 2),\n",
    "                        torch.cat([A, torch.zeros(nBatch, neq, nineq).type_as(Q_tilde)], 2)], 1)\n",
    "        g_ = torch.cat([rx, rs], 1)\n",
    "        h_ = torch.cat([rz, ry], 1)\n",
    "    else:\n",
    "        A_ = torch.cat(\n",
    "            [G, torch.eye(nineq).type_as(Q_tilde).repeat(nBatch, 1, 1)], 2)\n",
    "        g_ = torch.cat([rx, rs], 1)\n",
    "        h_ = rz\n",
    "\n",
    "    H_LU = lu_hack(H_)\n",
    "\n",
    "    invH_A_ = A_.transpose(1, 2).lu_solve(*H_LU)\n",
    "    invH_g_ = g_.unsqueeze(2).lu_solve(*H_LU).squeeze(2)\n",
    "\n",
    "    S_ = torch.bmm(A_, invH_A_)\n",
    "    S_ -= eps * torch.eye(neq + nineq).type_as(Q_tilde).repeat(nBatch, 1, 1)\n",
    "    S_LU = lu_hack(S_)\n",
    "    t_ = torch.bmm(invH_g_.unsqueeze(1), A_.transpose(1, 2)).squeeze(1) - h_\n",
    "    w_ = -t_.unsqueeze(2).lu_solve(*S_LU).squeeze(2)\n",
    "    t_ = -g_ - w_.unsqueeze(1).bmm(A_).squeeze()\n",
    "    v_ = t_.unsqueeze(2).lu_solve(*H_LU).squeeze(2)\n",
    "\n",
    "    dx = v_[:, :nz]\n",
    "    ds = v_[:, nz:]\n",
    "    dz = w_[:, :nineq]\n",
    "    dy = w_[:, nineq:] if neq > 0 else None\n",
    "\n",
    "    return dx, ds, dz, dy\n",
    "\n",
    "\n",
    "def factor_solve_kkt(Q, D, G, A, rx, rs, rz, ry):\n",
    "    nineq, nz, neq, nBatch = get_sizes(G, A)\n",
    "\n",
    "    H_ = torch.zeros(nBatch, nz + nineq, nz + nineq).type_as(Q)\n",
    "    H_[:, :nz, :nz] = Q\n",
    "    H_[:, -nineq:, -nineq:] = D\n",
    "    if neq > 0:\n",
    "        A_ = torch.cat([torch.cat([G, torch.eye(nineq).type_as(Q).repeat(nBatch, 1, 1)], 2),\n",
    "                        torch.cat([A, torch.zeros(nBatch, neq, nineq).type_as(Q)], 2)], 1)\n",
    "        g_ = torch.cat([rx, rs], 1)\n",
    "        h_ = torch.cat([rz, ry], 1)\n",
    "    else:\n",
    "        A_ = torch.cat([G, torch.eye(nineq).type_as(Q)], 1)\n",
    "        g_ = torch.cat([rx, rs], 1)\n",
    "        h_ = rz\n",
    "\n",
    "    H_LU = lu_hack(H_)\n",
    "\n",
    "    invH_A_ = A_.transpose(1, 2).lu_solve(*H_LU)\n",
    "    invH_g_ = g_.unsqueeze(2).lu_solve(*H_LU).squeeze(2)\n",
    "\n",
    "    S_ = torch.bmm(A_, invH_A_)\n",
    "    S_LU = lu_hack(S_)\n",
    "    t_ = torch.bmm(invH_g_.unsqueeze(1), A_.transpose(1, 2)).squeeze(1) - h_\n",
    "    w_ = -t_.unsqueeze(2).lu_solve(*S_LU).squeeze(2)\n",
    "    t_ = -g_ - w_.unsqueeze(1).bmm(A_).squeeze()\n",
    "    v_ = t_.unsqueeze(2).lu_solve(*H_LU).squeeze(2)\n",
    "\n",
    "    dx = v_[:, :nz]\n",
    "    ds = v_[:, nz:]\n",
    "    dz = w_[:, :nineq]\n",
    "    dy = w_[:, nineq:] if neq > 0 else None\n",
    "\n",
    "    return dx, ds, dz, dy\n",
    "\n",
    "\n",
    "def solve_kkt(Q_LU, d, G, A, S_LU, rx, rs, rz, ry):\n",
    "    \"\"\" Solve KKT equations for the affine step\"\"\"\n",
    "    nineq, nz, neq, nBatch = get_sizes(G, A)\n",
    "\n",
    "    invQ_rx = rx.unsqueeze(2).lu_solve(*Q_LU).squeeze(2)\n",
    "    if neq > 0:\n",
    "        h = torch.cat((invQ_rx.unsqueeze(1).bmm(A.transpose(1, 2)).squeeze(1) - ry,\n",
    "                       invQ_rx.unsqueeze(1).bmm(G.transpose(1, 2)).squeeze(1) + rs / d - rz), 1)\n",
    "    else:\n",
    "        h = invQ_rx.unsqueeze(1).bmm(G.transpose(1, 2)).squeeze(1) + rs / d - rz\n",
    "\n",
    "    w = -(h.unsqueeze(2).lu_solve(*S_LU)).squeeze(2)\n",
    "\n",
    "    g1 = -rx - w[:, neq:].unsqueeze(1).bmm(G).squeeze(1)\n",
    "    if neq > 0:\n",
    "        g1 -= w[:, :neq].unsqueeze(1).bmm(A).squeeze(1)\n",
    "    g2 = -rs - w[:, neq:]\n",
    "\n",
    "    dx = g1.unsqueeze(2).lu_solve(*Q_LU).squeeze(2)\n",
    "    ds = g2 / d\n",
    "    dz = w[:, neq:]\n",
    "    dy = w[:, :neq] if neq > 0 else None\n",
    "\n",
    "    return dx, ds, dz, dy\n",
    "\n",
    "\n",
    "def pre_factor_kkt(Q, G, A):\n",
    "    \"\"\" Perform all one-time factorizations and cache relevant matrix products\"\"\"\n",
    "    nineq, nz, neq, nBatch = get_sizes(G, A)\n",
    "\n",
    "    try:\n",
    "        Q_LU = lu_hack(Q)\n",
    "    except:\n",
    "        raise RuntimeError(\"\"\"\n",
    "qpth Error: Cannot perform LU factorization on Q.\n",
    "Please make sure that your Q matrix is PSD and has\n",
    "a non-zero diagonal.\n",
    "\"\"\")\n",
    "\n",
    "    # S = [ A Q^{-1} A^T        A Q^{-1} G^T          ]\n",
    "    #     [ G Q^{-1} A^T        G Q^{-1} G^T + D^{-1} ]\n",
    "    #\n",
    "    # We compute a partial LU decomposition of the S matrix\n",
    "    # that can be completed once D^{-1} is known.\n",
    "    # See the 'Block LU factorization' part of our website\n",
    "    # for more details.\n",
    "\n",
    "    G_invQ_GT = torch.bmm(G, G.transpose(1, 2).lu_solve(*Q_LU))\n",
    "    R = G_invQ_GT.clone()\n",
    "    S_LU_pivots = torch.IntTensor(range(1, 1 + neq + nineq)).unsqueeze(0) \\\n",
    "        .repeat(nBatch, 1).type_as(Q).int()\n",
    "    if neq > 0:\n",
    "        invQ_AT = A.transpose(1, 2).lu_solve(*Q_LU)\n",
    "        A_invQ_AT = torch.bmm(A, invQ_AT)\n",
    "        G_invQ_AT = torch.bmm(G, invQ_AT)\n",
    "\n",
    "        LU_A_invQ_AT = lu_hack(A_invQ_AT)\n",
    "        P_A_invQ_AT, L_A_invQ_AT, U_A_invQ_AT = torch.lu_unpack(*LU_A_invQ_AT)\n",
    "        P_A_invQ_AT = P_A_invQ_AT.type_as(A_invQ_AT)\n",
    "\n",
    "        S_LU_11 = LU_A_invQ_AT[0]\n",
    "        U_A_invQ_AT_inv = (P_A_invQ_AT.bmm(L_A_invQ_AT)\n",
    "                           ).lu_solve(*LU_A_invQ_AT)\n",
    "        S_LU_21 = G_invQ_AT.bmm(U_A_invQ_AT_inv)\n",
    "        T = G_invQ_AT.transpose(1, 2).lu_solve(*LU_A_invQ_AT)\n",
    "        S_LU_12 = U_A_invQ_AT.bmm(T)\n",
    "        S_LU_22 = torch.zeros(nBatch, nineq, nineq).type_as(Q)\n",
    "        S_LU_data = torch.cat((torch.cat((S_LU_11, S_LU_12), 2),\n",
    "                               torch.cat((S_LU_21, S_LU_22), 2)),\n",
    "                              1)\n",
    "        S_LU_pivots[:, :neq] = LU_A_invQ_AT[1]\n",
    "\n",
    "        R -= G_invQ_AT.bmm(T)\n",
    "    else:\n",
    "        S_LU_data = torch.zeros(nBatch, nineq, nineq).type_as(Q)\n",
    "\n",
    "    S_LU = [S_LU_data, S_LU_pivots]\n",
    "    return Q_LU, S_LU, R\n",
    "\n",
    "\n",
    "factor_kkt_eye = None\n",
    "\n",
    "\n",
    "def factor_kkt(S_LU, R, d):\n",
    "    \"\"\" Factor the U22 block that we can only do after we know D. \"\"\"\n",
    "    nBatch, nineq = d.size()\n",
    "    neq = S_LU[1].size(1) - nineq\n",
    "    # TODO: There's probably a better way to add a batched diagonal.\n",
    "    global factor_kkt_eye\n",
    "    if factor_kkt_eye is None or factor_kkt_eye.size() != d.size():\n",
    "        # print('Updating batchedEye size.')\n",
    "        factor_kkt_eye = torch.eye(nineq).repeat(\n",
    "            nBatch, 1, 1).type_as(R).bool()\n",
    "    T = R.clone()\n",
    "    T[factor_kkt_eye] += (1. / d).squeeze().view(-1)\n",
    "\n",
    "    T_LU = lu_hack(T)\n",
    "\n",
    "    if not T.is_cuda:\n",
    "        # TODO: Don't use pivoting in most cases because\n",
    "        # torch.lu_unpack is inefficient here:\n",
    "        oldPivotsPacked = S_LU[1][:, -nineq:] - neq\n",
    "        oldPivots, _, _ = torch.lu_unpack(\n",
    "            T_LU[0], oldPivotsPacked, unpack_data=False)\n",
    "        newPivotsPacked = T_LU[1]\n",
    "        newPivots, _, _ = torch.lu_unpack(\n",
    "            T_LU[0], newPivotsPacked, unpack_data=False)\n",
    "\n",
    "        # Re-pivot the S_LU_21 block.\n",
    "        if neq > 0:\n",
    "            S_LU_21 = S_LU[0][:, -nineq:, :neq]\n",
    "            S_LU[0][:, -nineq:,\n",
    "                    :neq] = newPivots.transpose(1, 2).bmm(oldPivots.bmm(S_LU_21))\n",
    "\n",
    "        # Add the new S_LU_22 block pivots.\n",
    "        S_LU[1][:, -nineq:] = newPivotsPacked + neq\n",
    "\n",
    "    # Add the new S_LU_22 block.\n",
    "    S_LU[0][:, -nineq:, -nineq:] = T_LU[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SvEpyu8pryp6"
   },
   "outputs": [],
   "source": [
    "def qpth_opt(Q,p,G,h,A,b):\n",
    "\n",
    "  start=time.time()\n",
    "  Q_LU, S_LU, R = pre_factor_kkt(Q, G, A)\n",
    "  x,s,z,y=forward(Q, p, G, h, A, b, Q_LU, S_LU, R, eps=1e-12, verbose=0, notImprovedLim=3,\n",
    "              maxIter=20, solver=KKTSolvers.LU_FULL)\n",
    "  return x,s,z,y\n",
    "  # print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rPwWtVSjqrzK"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9rWI2ZFtqzWR"
   },
   "outputs": [],
   "source": [
    "# df=pd.read_csv('gdrive/My Drive/test_cases.csv')\n",
    "# # df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Smh7HXiiq7CK"
   },
   "outputs": [],
   "source": [
    "# ip=\"csv\"\n",
    "# nbatch=len(df)\n",
    "# nx=6\n",
    "# nineq=6\n",
    "# neq=3\n",
    "# Q=[]\n",
    "# p=[]\n",
    "# G=[]\n",
    "# h=[]\n",
    "# A=[]\n",
    "# b=[]\n",
    "# for i in range(nbatch):\n",
    "#   seed=int(df[\"seed\"][i])\n",
    "#   random.seed(seed)\n",
    "#   Q.append(make_spd_matrix(nx,random_state=seed).reshape(1,-1))\n",
    "#   p.append([random.randint(0,9) for i in range(nx)])#.astype(float))\n",
    "#   G.append([random.randint(0,9)*((-1)**random.randint(0,1)) for i in range(nineq*nx)])#.astype(float))\n",
    "#   h.append([0 for i in range(nineq)])#.astype(float))\n",
    "#   A.append([random.randint(0,9) for i in range(neq*nx)])#.astype(float))\n",
    "#   b.append([random.randint(0,9) for i in range(neq)])#.astype(float)  )\n",
    "\n",
    "# Q=torch.tensor(Q).view(nbatch,nx,nx).type(torch.DoubleTensor)\n",
    "# p=torch.tensor(p).view(nbatch,nx).type(torch.DoubleTensor)\n",
    "# G=torch.tensor(G).view(nbatch,nineq,nx).type(torch.DoubleTensor)\n",
    "# h=torch.tensor(h).view(nbatch,nineq).type(torch.DoubleTensor)\n",
    "# A=torch.tensor(A).view(nbatch,neq,nx).type(torch.DoubleTensor)\n",
    "# b=torch.tensor(b).view(nbatch,neq).type(torch.DoubleTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FXGoigg3sdBL"
   },
   "outputs": [],
   "source": [
    "    # cols=[str(i)+\"copy\" for i in range(nx)]\n",
    "    # error_cp=np.zeros(nbatch)\n",
    "    # error_qp=np.zeros(nbatch)\n",
    "    # new=pd.DataFrame(x.numpy().round(6).reshape(nbatch,-1),columns=cols)\n",
    "    # tmp=df.copy()\n",
    "    # for i,col in enumerate(cols):\n",
    "    #   tmp[col]=new[col].copy()\n",
    "    #   error_cp+=np.round(tmp[str(i)+\"cp\"],6)-np.round(tmp[col],6)\n",
    "    #   error_qp+=np.round(tmp[str(i)+\"qpt\"],6)-np.round(tmp[col],6)\n",
    "\n",
    "    # tmp[\"cp-copy-error\"]=error_cp\n",
    "    # tmp[\"cp-copy-er_flag\"]=error_cp!=0\n",
    "    # tmp[\"qpt-copy-error\"]=error_qp\n",
    "    # tmp[\"qpt-copy-er_flag\"]=error_qp!=0\n",
    "    # print(\"No.of problems solved: \",len(tmp))\n",
    "    # print(\"No.of errors from copy Method: \",len(error_cp[error_cp!=0]),\" (\",round(len(error_cp[error_cp!=0])*100/len(tmp),2),\")\")\n",
    "    # print(\"No.of errors from qpth: \",len(tmp[tmp[\"cp-qpt-er_flag\"]]),\" (\",round(len(tmp[tmp[\"cp-qpt-er_flag\"]])*100/len(tmp),2),\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "W99t8fKos8EW"
   },
   "outputs": [],
   "source": [
    "# tmp[tmp[\"cp-copy-er_flag\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3eQqhDNDtaMe"
   },
   "outputs": [],
   "source": [
    "# qpf = QPFunction()\n",
    "# solution = qpf(Q, p, G, h, A, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "se2YkRI4ux_U"
   },
   "outputs": [],
   "source": [
    "    # x=solution\n",
    "    # cols=[str(i)+\"copy\" for i in range(nx)]\n",
    "    # error_cp=np.zeros(nbatch)\n",
    "    # error_qp=np.zeros(nbatch)\n",
    "    # new=pd.DataFrame(x.numpy().round(6).reshape(nbatch,-1),columns=cols)\n",
    "    # tmp=df.copy()\n",
    "    # for i,col in enumerate(cols):\n",
    "    #   tmp[col]=new[col].copy()\n",
    "    #   error_cp+=np.round(tmp[str(i)+\"cp\"],6)-np.round(tmp[col],6)\n",
    "    #   error_qp+=np.round(tmp[str(i)+\"qpt\"],6)-np.round(tmp[col],6)\n",
    "\n",
    "    # tmp[\"cp-copy-error\"]=error_cp\n",
    "    # tmp[\"cp-copy-er_flag\"]=error_cp!=0\n",
    "    # tmp[\"qpt-copy-error\"]=error_qp\n",
    "    # tmp[\"qpt-copy-er_flag\"]=error_qp!=0\n",
    "    # print(\"No.of problems solved: \",len(tmp))\n",
    "    # print(\"No.of errors from copy Method: \",len(error_cp[error_cp!=0]),\" (\",round(len(error_cp[error_cp!=0])*100/len(tmp),2),\")\")\n",
    "    # print(\"No.of errors from qpth: \",len(tmp[tmp[\"cp-qpt-er_flag\"]]),\" (\",round(len(tmp[tmp[\"cp-qpt-er_flag\"]])*100/len(tmp),2),\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oqAg4gRFu44D"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMyyQ6/NVIr41kKDOZk/3JA",
   "include_colab_link": true,
   "name": "qpth_class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
