\chapter{Primal-Dual Interior Point Methods}\label{chapter:PDIPM}
\par The primal-dual interior point methods are proven to be a class of algorithms that solves a wide range of optimization problems including linear-programming-problems, convex-quadratic-programming-problems, semi-definite-programming etc.\cite{wright1997primal}. They fall under the category of interior point methods and employ the Newton's method to solve optimization problems in an efficient manner. The research works that gave rise to the field of \textit{Interior-Point Methods} and successively the \textit{Primal-Dual Methods} started with the publication of Karmarkar's paper \cite{karmarkar1984new} which marked an important point in the history of optimization algorithms. This chapter walks through the basic building blocks of the primal-dual interior point methods, its evolution as a path-following algorithm, the Mehrotra's predictor-corrector approach and finally looks at the complexity analysis of these algorithms and compare it with other common optimization algorithms like the simplex method.
\section{Newton's Method}\label{NM}
\par The Newton's method finds the roots of a function by moving along search directions generated by the the liner approximation of the same function starting from a point in the functions domain. From the Taylor's theorem \cite{apostol1964mathematical} the linear approximation of a function \(f\) which is differentiable at a point \(x_0\) is given by:
\begin{equation}\label{lin appx}
\Bar{f}(x)=f(x_0)+(x-x_0)f'(x_0)
\end{equation}
\par The Newton Step \( \Delta x_n\) can be obtained by finding the root of this linear approximation of \(f\) as $f'(x_n) \Delta x_n=-f(x_n)$.In the matrix system the equivalent representation to obtain the newton step would be {\(J(x_n) \Delta x_n=-F(x_n)\)} where \(J(x_n)\) is the Jacobian of \(F(x)\) at \(x_n\). These newton steps are the search directions along which the the linear approximation of the function will be iteratively evaluated. The Newton Iterates are hoped to converge to the roots of the function \(F(x)\)\cite{apostol1964mathematical}. \par
The primal-dual interior-point methods, like it's name says, solves the primal and it's corresponding dual problem in each iteration by exploiting the Newton Method. The function which the Newton Method tries to solve is comprised of the Karush-Kuhn-Tucker (KKT) equality conditions (or its modifications). For constrained quadratic problems, the KKT conditions comprise a set of linear equations which can easily solved by the Newton's method. The underlying idea is that, for any convex optimization problem with differentiable objective and constraint functions, any points that satisfy the KKT conditions are primal and dual optimal and have zero duality gap\cite{boyd2004convex}. 
\section{Interior Point Methods and PDIPM's}\label{IPM}
\par The interior-point methods get its name from the fact that the iterates generated by this method are points in the interior of the feasible region - i.e, the iterates generated are strictly feasible (or satisfies the inequality constraints strictly). This is in contrast to the Simplex method, which moves along the edges of the feasible region. The primal-dual interior-point methods generates these strictly feasible by solving the equality conditions of the KKT system using Newton Method, restricting the iterates to satisfy the inequality conditions of the KKT system strictly.\par
Consider a QP(with slack variable \(s\)) and its associated KKT conditions for optimality, as mentioned in section \ref{qp_back} :
\begin{equation}\label{qp-eqn}
\begin{aligned}
&\text { minimize } \quad(1 / 2) x^{T} Q x+q^{T} x\\
&\text { subject to } \quad G x+s=h,  s \geq 0\\
& \hspace{60pt} \quad A x=b, 
\end{aligned}
\end{equation}
\hspace{30pt}where $Q\in \mathbf{S}_{+}^{n}$, $q \in \mathbf{R}^{n}$, $G \in \mathbf{R}^{p \times n}$, $h \in \mathbf{R}^{p}$, $A \in \mathbf{R}^{m \times n}$ and $b \in \mathbf{R}^{m}$  is the problem data and $x \in \mathbf{R}^{n}$ and $s \in \mathbf{R}^{p}$. are the variables. Considering the dual variables as $y \in \mathbf{R}^{m}$ (associated with the equality constraints) and $z \in \mathbf{R}^{p}$ (associated with the inequality constraints), the KKT conditions for optimality of this problem are:
\begin{itemize}
\setlength\itemsep{0}
\item $Q x+q+G^{T} z+A^{T} y=0$ \hfill{Stationarity}
\item $z_{i} s_{i}=0, \quad i=1, \ldots, p$ \hfill{Complementary Slackness}
\item $G x+s=h, \quad A x=b$, $s \geq 0,$ \hfill{Primal Feasibility}
\item $z \geq 0$ \hfill{Dual Feasibility}
\end{itemize}\par
The Primal-Dual Interior-Point methods, as mentioned before, find the solution to the primal and dual problem by solving this KKT system using the Newton Method. The function $F$ that the Newton method attempts to solve is comprised of the equality conditions in the KKT system given above. 
\begin{equation}\label{newton-fn}
F(x,s,z,y)=\left[\begin{array}{c}-\left(A^{T} y+G^{T} z+Q x+q\right) \\ -S z \\ -(G x+s-h) \\ -(A x-b)\end{array}\right]
\end{equation}
\hspace{30pt}where $S=\operatorname{diag}(s)$. \par
Now, the update steps can be obtained by the Newton Method as {\(J(x_n) \Delta x_n=-F(x_n)\)}, precisely here as:
\begin{equation} \label{KKT}
\left[\begin{array}{cccc}Q & 0 & G^{T} & A^{T} \\ 0 & Z & S & 0 \\ G & I & 0 & 0 \\ A & 0 & 0 & 0\end{array}\right]\left[\begin{array}{c}\Delta x \\ \Delta s \\ \Delta z \\ \Delta y\end{array}\right]=\left[\begin{array}{c}-\left(A^{T} y+G^{T} z+Q x+q\right) \\ -S z \\ -(G x+s-h) \\ -(A x-b)\end{array}\right]
\end{equation}
\hspace{30pt}where $Z=\operatorname{diag}(z)$\cite{mattingley2012cvxgen}.\par
The Jacobian Matrix on the left hand side is obtained by partially differentiating the function $F(x,s,z,y)$ with respect to $x,s,z$ and $y$ respectively.\par
All primal-dual methods generate iterates $(x,s,z,y)$ that satisfy the KKT inequality conditions strictly, that is, $s>0$ and $z>0$. By respecting these conditions, the methods avoid spurious solutions, which are points that satisfy $F(x,s,z,y)=0$ but not $(s,z) \geq 0$ \cite{wright1997primal}. Most interior-point methods actually require the iterates to be \textit{strictly feasible}: i.e, each $(x,s,z,y)$ must satisfy the linear equality constraints for the primal and dual problems. We can define $\mathcal{F}$ as the primal-dual \textit{feasible set} and $\mathcal{F}^0$ as the \textit{strictly feasible set} by:
\begin{nscenter}
$\mathcal{F} =\{(x,s,z,y)|Q x+q+G^{T} z+A^{T} y=0, G x+s=h, A x=b, (s,z)\geq0\} $
$\mathcal{F}^{0} =\{(x,s,z,y)|Q x+q+G^{T} z+A^{T} y=0, G x+s=h, A x=b, (s,z)>0\} $
\end{nscenter}\par
If the current point is strictly feasible, then the terms on the right hand side of equation\ref{KKT} vanishes to zero, except the term $-Sz$. However, since it is difficult to find a strictly feasible starting point, the \textit{infeasible interior-point} methods does not require the starting points to belong to the set $\mathcal{F}^0$, rather it only requires the positivity of $s$ and $z$, i.e., $(s,z)>0$ and solves the system of equations in equation\ref{KKT}.\par
Once the Newton's step is obtained, instead of taking a full step along this newton direction, the best possible \textit{step-size} that can be made along this step direction without violating the inequality consition $(s,z>0)$ is found using a line search. The updates are then performed as:
\begin{equation}
    (x^{k+1},s^{k+1},z^{k+1},y^{k+1})=(x^{k},s^{k},z^{k},y^{k})+\alpha*(\Delta x^{k},\Delta s^{k},\Delta z^{k},\Delta y^{k})
\end{equation}
\hspace{30pt} for some line search parameter $\alpha \in (0,1] $. \par
Unfortunately, the steps that can be taken along this pure Newton-Directions are very small before violating the inequality condition, and hence makes very little progress towards the solution \cite{wright1997primal}. This is taken care of by biasing the the search direction - which is discussed in the following section.
\section{Path-Following Algorithms}\label{PFA}
\par Primal-Dual Interior-Point Methods modify the basic Newton's method by biasing the search direction towards the interior of the search space (the non-negative orthant $(s,z)\geq0$). This is obtained by slightly modifying the KKT conditions by introducing a term $\tau$ in the \textit{complementary slackness} condition.
\begin{equation}
    x_is_i=\tau
\end{equation}
\hspace{30pt} The points in $\mathcal{C}={(x_\tau,s_\tau,z_\tau,y_\tau)|\tau>0}$ is called the \textbf{Central Path}. This modified KKT system would approximate the original KKT system as $\tau$ goes to zero. Also as $\tau \shortdownarrow 0$, if $\mathcal{C}$ converges to anything, then this is the optimal - primal-dual solution. The central path drives us to the solution at a much faster rate than the basic Newton Method, since the steps taken are longer in the former compared to the later. The biased search direction $\tau$ is defined by using a \textit{centering parameter} $\sigma\in[0,1]$ and a \textit{duality measure} $\mu$ defined as given below: \cite{wright1997primal}. 
\begin{equation}
    \mu= s^{T} z / p
\end{equation}\par
Therefore, the modified KKT system is given by:
\begin{equation} \label{KKT_mod}
\left[\begin{array}{cccc}Q & 0 & G^{T} & A^{T} \\ 0 & Z & S & 0 \\ G & I & 0 & 0 \\ A & 0 & 0 & 0\end{array}\right]\left[\begin{array}{c}\Delta x \\ \Delta s \\ \Delta z \\ \Delta y\end{array}\right]=\left[\begin{array}{c}-\left(A^{T} y+G^{T} z+Q x+q\right) \\ -S z + \boldsymbol{\sigma \mu e}\\ -(G x+s-h) \\ -(A x-b)\end{array}\right]
\end{equation}
\par When $\sigma=0$ the above system of equations resembles equation\ref{KKT}. The steps then generated are the Standard Newton step often called the \textit{affine-scaling direction}. When $\sigma=1$ the equation \ref{KKT_mod} defines a \textit{centering direction}. This gives points on the central path, but they help very little in reducing $\mu$ significantly. However they set the scene for major progress in the next iteration. To achieve the twin goals of reducing $\mu$ and improving centrality, many algorithms use values of $\sigma$ in the interval (0,1)\cite{wright1997primal}.
\par These algorithms that restrict the iterates to a neighborhood of the central path $\mathcal{C}$ and follows the central path to reach the solution are called \textbf{Path-Following Algorithms}. Some of the common path following algorithms include \textit{short-step} path following methods, \textit{Long-step} path following methods, \textit{predictor-corrector} method etc. The Mehrotra's Predictor Corrector algorithm \cite{mehrotra1992implementation} is one such class of algorithms which has shown consistent performance in solving a wide range of problems. This method is described in the following section.
\section{Mehrotra's Predictor-Corrector Method}
The Mehrotra's predictor-corrector method\cite{mehrotra1992implementation} generates iterates which satisfy the positivity conditions, but might not belong to the strictly feasible set $\mathcal{F}^0$ mentioned before. This method is characterized by the following three components\cite{wright1997primal} :
\begin{itemize}
    \item a pure newton step or an affine-scaling "predictor" direction
    \item a centering parameter $\sigma$ which is adaptively chosen based on the affine-newton step
    \item a "corrector" direction that compensate for some of the non-linearity in the affine-scaling direction.
\end{itemize}
\par Unlike the standard path-following primal-dual methods \ref{KKT_mod}, Mehrotra's Predictor Corrector Method computes the affine-scaling direction \ref{KKT} and the centering term $\sigma \mu e$ separately. This is done so, because the centering parameter $\sigma$ is chosen adaptively based on the improvement the affine-scaling direction(generated by the pure newton method) offers. If the affine-scaling direction makes a good progress in reducing the duality measure $\mu$ without violating the inequality bounds $(s,z)>0$, then there is no much centering required, and so $\sigma$ can be set close to zero for this iteration. On the other hand, if very little progress can be made along the affine-scaling direction, the centering parameter $\sigma$ is set close to 1 so as to drive the iterates closer to the central path so that they can make significant progress through the next iterations. One disadvantage of splitting the system into two systems is that, two linear systems are to be solved in each iteration. But however, this can be done efficiently, since the coefficient matrix is the same in both the cases and needs to be factorized only once.
\par The affine-scaling directions are obtained from a linear approximation to the equality conditions in the KKT system as described by equation \ref{lin appx}. The affine-scaling direction can be used to assess the error in the linear approximation. This error is used to calculate a \textit{corrector }component - this improves the linear, first-order model of $F$ to a quadratic, second-order model. 
\subsection{Outline of Mehrotra's Predictor Corrector Algorithm}
\textbf{Solving the KKT system}\\
It is important to note that the coefficient matrix in the KKT system has a special structure. If we have a closer look at the coefficient matrix of the KKT system \ref{KKT}, we realize that, all the matrix entries except $Z$ and $S$ (in the second row) remain unchanged across iterations. This can be utilized to improve the efficiency of the solve steps of the KKT system. This matrix can be symmetrized by dividing the whole second row to obtain the following:
\begin{equation} \label{KKT_sym}
\left[\begin{array}{cc|cc}Q & 0 & G^{T} & A^{T} \\ 0 & S^{-1}Z & I & 0 \\\hline G & I & 0 & 0 \\ A & 0 & 0 & 0\end{array}\right]\left[\begin{array}{c}\Delta x \\ \Delta s \\ \Delta z \\ \Delta y\end{array}\right]=\left[\begin{array}{c}-\left(A^{T} y+G^{T} z+Q x+q\right) \\ - z \\ -(G x+s-h) \\ -(A x-b)\end{array}\right]
\end{equation}\\
where $S^{-1}Z$ is a diagonal matrix, represented as $D=\operatorname{diag}(d)$ where $d=z/s$.\par
In each iteration of the Mehrotra's Predictor-Corrector algorithm, systems similar to the above with same coefficient matrix on the left hand side but different right hand side would be solved. A solver method that efficiently does this solving procedure by a suitable factorization of the coefficient matrix (once per iteration) would help improve the performance of the algorithm.\\ \\
%TODO: add more details on the factorization + solver procedure
\textbf{Initialization}\\
Mehrotra's predictor corrector algorithm works with infeasible iterates and only requires the positivity condition $(s,z)>0$. Hence an initial point can be generated in many ways, one among which is given below as described in \cite{vandenberghe2010cvxopt}. This method tries to find the (analytic) solution of the primal and dual problems :
$$
\begin{array}{ll}
\text { minimize } & (1 / 2) x^{T} Q x+q^{T} x+(1 / 2)\|s\|_{2}^{2} \\
\text { subject to } & G x+s=h, \quad A x=b
\end{array}
$$
with variables $x$ and $s,$ and
$$
\text { maximize } \quad-(1 / 2) w^{T} Q w-h^{T} z-b^{T} y-(1 / 2)\|z\|_{2}^{2}
$$
$$
\text { subject to } \quad Q w+q+G^{T} z+A^{T} y=0
$$
with variables $w, y$ and $z .$ The solution to the KKT system of these problems would reduce to:
$$
\left[\begin{array}{ccc}
Q & G^{T} & A^{T} \\
G & -I & 0 \\
A & 0 & 0
\end{array}\right]\left[\begin{array}{l}
x \\
z \\
y
\end{array}\right]=\left[\begin{array}{c}
-q \\
h \\
b
\end{array}\right]
$$
If we have a solver to solve the general KKT system across iterations, we can use that solver 
to find solutions to the problems above by passing in the matrix system given below which resembles the structure of \ref{KKT_sym}.
\begin{equation} \label{KKT_sym}
\left[\begin{array}{cccc}Q & 0 & G^{T} & A^{T} \\ 0 & I & I & 0\\  G & I & 0 & 0 \\ A & 0 & 0 & 0\end{array}\right]\left[\begin{array}{c}\Delta x \\ \Delta s \\ \Delta z \\ \Delta y\end{array}\right]=\left[\begin{array}{c}-q \\ 0 \\ h \\ b\end{array}\right]
\end{equation}
The initial primal and dual variables $x$ and $y$ are set to $x^{(0)}=x$ and $y^{(0)}=y .$ 
However, we require the positivity of the variable $s$ and $z$. For achieving this, we set $z=G x-h$ and $\alpha_{p}=\inf \{\alpha |-z+\alpha 1 \geq 0\},$ and use
$$
s^{(0)}=\left\{\begin{array}{ll}
-z & \alpha_{p}<0 \\
-z+\left(1+\alpha_{p}\right) \mathbf{1} & \text { otherwise }
\end{array}\right.
$$
as the initial value of $s .$ Finally, we set $\alpha_{d}=\inf \{\alpha | z+\alpha \mathbf{1} \geq 0\},$ and use
$$
z^{(0)}=\left\{\begin{array}{ll}
z & \alpha_{d}<0 \\
z+\left(1+\alpha_{d}\right) \mathbf{1} & \text { otherwise }
\end{array}\right.
$$
as the initial value of $z .$ 
This gives the starting point $\left(x^{(0)}, s^{(0)}, z^{(0)}, y^{(0)}\right)$.\\ \\
\textbf{Main Iterations}\\
1. The optimum can be said to be achieved once the residuals and the duality gap approaches zero. This can be used as a early stopping criterion, for avoiding unnecessary computations until max\_iteration is reached. (The residuals are the difference of the KKT equality conditions from zero.Precisely $-(A^{T} y+G^{T} z+Q x+q)$ is the dual residual and $-(G x+s-h) & -(A x-b)$ form the primal residual.)\\ \\
2. The affine-scaling directions are found using pure-newton step as:
\begin{equation}
\left[\begin{array}{cccc}
Q & 0 & G^{T} & A^{T} \\
0 & Z & S & 0 \\
G & I & 0 & 0 \\
A & 0 & 0 & 0
\end{array}\right]\left[\begin{array}{c}
\Delta x^{a f f} \\
\Delta s^{a f f} \\
\Delta z^{a f f} \\
\Delta y^{a f f}
\end{array}\right]=\left[\begin{array}{c}
-\left(A^{T} y+G^{T} z+Q x+q\right) \\
-S z \\
-(G x+s-h) \\
-(A x-b)
\end{array}\right]   
\end{equation}\\ \\
3. Using these affine-scaling directions obtained above, the centering parameter $\sigma$ is calculated as:\\
\begin{equation}
\boldsymbol{\sigma}=\left(\frac{\left(s+\alpha \Delta s^{\mathrm{aff}}\right)^{T}\left(z+\alpha \Delta z^{\mathrm{aff}}\right)}{s^{T} z}\right)^{3}=\left(\frac{ \mu^{\mathrm{aff}}}{\mu}\right)^{3} 
\\
\end{equation}
\hspace{30pt} where, 
\begin{equation}
\alpha=\sup \left\{\alpha \in[0,1] | s+\alpha \Delta s^{\mathrm{aff}} \geq 0, z+\alpha \Delta z^{\mathrm{aff}} \geq 0\right\}
\end{equation}
i.e, the centering parameter is adaptively chosen based on the improvement offered by the affine-scaling direction alone. $\mu^{aff}$ refers to the new duality measure if only the affine-scaling direction was used for update. 
Also the duality measure $\mu$ is calculated as $\boldsymbol{\mu}=s^Tz/p$.\\ \\
4. The centering parameter and the duality measure can be used to determine the centering term that biases the search direction. We can combine this centering step with the corrector step that accounts for the non-linearity in the affine-step. The combined centering-corrector steps are obtained by solving the following system:
\begin{equation}
\left[\begin{array}{cccc}
Q & 0 & G^{T} & A^{T} \\
0 & Z & S & 0 \\
G & I & 0 & 0 \\
A & 0 & 0 & 0
\end{array}\right]\left[\begin{array}{c}
\Delta x^{c e} \\
\Delta s^{c c} \\
\Delta z^{c c} \\
\Delta y^{c c}
\end{array}\right]=\left[\begin{array}{c}
0 \\
\sigma \mu 1-\operatorname{diag}\left(\Delta s^{a f f}\right) \Delta z^{a f f} \\
0 \\
0
\end{array}\right]
\end{equation}\\ \\
5. Once the affine-steps and the centering-corrector-steps are obtained, the update-steps can be calculated as:
\begin{equation}
\begin{aligned}
&\Delta x=\Delta x^{2 / f}+\Delta x^{c c}\\
&\begin{array}{l}
\Delta s=\Delta s^{\text {aff }}+\Delta s^{\mathrm{cc}} \\
\Delta y=\Delta y^{\text {aff }}+\Delta y^{\mathrm{cc}} \\
\Delta z=\Delta z^{\text {aff }}+\Delta z^{\infty}
\end{array}
\end{aligned}
\end{equation}\\ \\
6. Finally we perform the updates of the variables as:
\begin{equation}
\begin{aligned}
&x:=x+\alpha \Delta x\\
&s:=s+\alpha \Delta s\\
&y:=y+\alpha \Delta y\\
&z:=z+\alpha \Delta z
\end{aligned}
\end{equation}
\hspace{30pt} where
\begin{equation}
    \alpha=\min \{1,0.99 \sup \{\alpha \geq 0 | s+\alpha \Delta s \geq 0, z+\alpha \Delta z \geq 0\}\}
\end{equation}\\ 
6. Repeat this steps from 1. until max\_iteration is reached

\section{Convergence Analysis of PDIPM's}
\section{Complexity Analysis of PDIPM's}


