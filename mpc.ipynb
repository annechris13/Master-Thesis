{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class mpc_class():\n",
    "    # TODO: fix these linting erros instead of ignoring them\n",
    "    # pylint: disable=attribute-defined-outside-init\n",
    "    def __init__(self,max_iter=20):\n",
    "        self.max_iter=max_iter\n",
    "\n",
    "    def solve(self,Q,q,G,h,A,b,refine=False,print_warning=False,check_Q_psd=True):\n",
    "        self.Q=Q\n",
    "        self.q=q\n",
    "        self.G=G\n",
    "        self.h=h\n",
    "        self.G_T=torch.transpose(self.G,dim0=2,dim1=1)\n",
    "        self.A=A\n",
    "        self.b=b\n",
    "        if self.A != None:\n",
    "            self.A_T=torch.transpose(self.A,dim0=2,dim1=1)\n",
    "        self.nbatch, self.nx, self.nineq, self.neq = self.get_sizes()\n",
    "        self.device= self.Q.device\n",
    "        self.dtype=self.Q.dtype\n",
    "        if check_Q_psd: #necessary for qp to be convex\n",
    "            self.is_Q_pd()\n",
    "        self.refine=refine\n",
    "        self.J=self.get_Jacobian()\n",
    "        self.J=self.get_lu_J()\n",
    "\n",
    "        #get initial solution\n",
    "        if self.neq!=None:\n",
    "            self.b_unsqueezed=self.b.unsqueeze(-1)\n",
    "        else:\n",
    "            self.b_unsqueezed=None\n",
    "        self.x,self.s,self.z,self.y=self.solve_kkt(-self.q.unsqueeze(-1),\n",
    "                                                   torch.zeros((self.nbatch,self.nineq)\n",
    "                                                   ,device=self.device, dtype=self.dtype).unsqueeze(-1),\n",
    "                                                   self.h.unsqueeze(-1),self.b_unsqueezed)\n",
    "\n",
    "        alpha_p=self.get_initial(-self.z)\n",
    "        alpha_d=self.get_initial(self.z)\n",
    "        self.s=-self.z+alpha_p*(torch.ones_like(self.z))\n",
    "        self.z=self.z+alpha_d*(torch.ones_like(self.z))\n",
    "\n",
    "        #main iterations\n",
    "        self.x,self.s,self.z,self.y=self.mpc_opt(print_warning=print_warning)\n",
    "        op_val=0.5*torch.bmm(torch.transpose(self.x,dim0=2,dim1=1),\n",
    "                             torch.bmm(self.Q,self.x))+torch.bmm(\n",
    "                             torch.transpose(self.q.unsqueeze(-1),dim0=2,dim1=1),self.x)\n",
    "        return self.x, op_val\n",
    "\n",
    "    def get_sizes(self):\n",
    "        #2 dimensions ==> dimensions are (ninenq,nx), add dimension nbatch at pos 0\n",
    "        if(self.Q.dim()==self.G.dim()==2):\n",
    "            self.Q=self.Q.unsqueeze(0)\n",
    "            self.q=self.q.unsqueeze(0)\n",
    "            self.G=self.G.unsqueeze(0)\n",
    "            self.h=self.h.unsqueeze(0)\n",
    "        if (self.A is not None) and (self.A.dim()==2):\n",
    "            self.A=self.A.unsqueeze(0)\n",
    "            self.b=self.b.unsqueeze(0)\n",
    "        #get sizes\n",
    "        nbatch, nineq, nx = self.G.size()\n",
    "        if self.A is not None:\n",
    "            _,neq,_=self.A.size()\n",
    "        else:\n",
    "            neq=None\n",
    "        return nbatch,nx,nineq,neq\n",
    "\n",
    "    def is_Q_pd(self):\n",
    "        try:\n",
    "            torch.cholesky(self.Q)\n",
    "        except:\n",
    "            raise RuntimeError(\"Q is not PD\")\n",
    "\n",
    "    def lu_factorize(self,x):\n",
    "        #do lu factorization of x\n",
    "        #avoid pivoting when possible, i.e when on cuda\n",
    "        data, pivots = x.lu(pivot=not x.is_cuda)\n",
    "        #define pivot matrix manually when on cuda\n",
    "        if x.is_cuda==True:\n",
    "            #pivot matrix doesnt do any pivoting\n",
    "            pivots = torch.arange(1, 1+x.size(1),dtype=torch.int,\n",
    "            device=self.device).unsqueeze(0).repeat(x.size(0), 1)\n",
    "        return (data, pivots)\n",
    "\n",
    "    def get_diag_matrix(self,d):\n",
    "        #return diagonal matrix with diagonal entries d\n",
    "        nBatch, n, _ = d.size()\n",
    "        Diag = torch.zeros((nBatch, n, n),device=self.device, dtype=self.dtype)\n",
    "        I = torch.eye(n,device=self.device, dtype=self.dtype).repeat(nBatch, 1, 1).bool()\n",
    "        Diag[I] = d.view(-1)\n",
    "        return Diag\n",
    "\n",
    "    def get_Jacobian(self):\n",
    "        #get the jacobian kkt matrix as concatenation of 4 blocks, B2=transpose(B3)\n",
    "        B1=torch.zeros((self.nbatch,self.nx+self.nineq,self.nx+self.nineq),device=self.device,dtype=self.dtype)\n",
    "        if self.neq==None:\n",
    "            B3=torch.zeros((self.nbatch,self.nineq,self.nx+self.nineq),device=self.device,dtype=self.dtype)\n",
    "            B4=torch.zeros((self.nbatch,self.nineq,self.nineq),device=self.device,dtype=self.dtype)\n",
    "        else:\n",
    "            B3=torch.zeros((self.nbatch,self.neq+self.nineq,self.nx+self.nineq),device=self.device,dtype=self.dtype)\n",
    "            B4=torch.zeros((self.nbatch,self.neq+self.nineq,self.neq+self.nineq),device=self.device,dtype=self.dtype)\n",
    "\n",
    "        B1[:,:self.nx,:self.nx]=self.Q\n",
    "        #D here is unit identity matrix (initial case)\n",
    "        self.D=torch.eye((self.nineq),device=self.device,dtype=self.dtype).repeat(self.nbatch,1,1)\n",
    "        B1[:,-self.nineq:,-self.nineq:]=self.D\n",
    "\n",
    "        B3[:,:self.nineq,:self.nx]=self.G\n",
    "        if self.A!=None:\n",
    "            B3[:,-self.neq:,:self.nx]=self.A\n",
    "        B3[:,:self.nineq,-self.nineq:]=torch.eye((self.nineq),device=self.device,dtype=self.dtype).repeat(self.nbatch,1,1)\n",
    "\n",
    "        B2=torch.transpose(B3, dim0=2, dim1=1)\n",
    "        size= self.nx+(2*self.nineq)\n",
    "        if self.neq!=None:\n",
    "            size+=self.neq\n",
    "        self.J= torch.zeros((self.nbatch,size,size),device=self.device, dtype=self.dtype)\n",
    "        self.J[:,:self.nx+self.nineq,:self.nx+self.nineq]=B1\n",
    "        self.J[:,:self.nx+self.nineq,self.nx+self.nineq:]=B2\n",
    "        self.J[:,self.nx+self.nineq:,:self.nx+self.nineq]=B3\n",
    "        self.J[:,self.nx+self.nineq:,self.nx+self.nineq:]=B4\n",
    "        # self.J=torch.cat((torch.cat((B1,B2),dim=2),torch.cat((B3,B4),dim=2)),dim=1)\n",
    "        return self.J\n",
    "\n",
    "    def get_lu_J(self,d=None):\n",
    "        # the jacobian J is modified when d is specified\n",
    "        if d!=None:\n",
    "            self.D=self.get_diag_matrix(d)\n",
    "            self.J[:,self.nx:self.nx+self.nineq,self.nx:self.nx+self.nineq]=self.D\n",
    "        self.J_lu,self.J_piv= self.lu_factorize(self.J)\n",
    "        return self.J\n",
    "\n",
    "    def solve_kkt(self,rx,rs,rz,ry):\n",
    "        # solve the KKT system with jacobian J and F specified by rx,rs,rz,ry\n",
    "        if ry!=None:\n",
    "            F=torch.cat((rx,rs,rz,ry), dim=1)\n",
    "        else:\n",
    "            F=torch.cat((rx,rs,rz), dim=1)\n",
    "        step=F.lu_solve(self.J_lu,self.J_piv)\n",
    "        dx=step[:,:self.nx,:]\n",
    "        ds=step[:,self.nx:self.nx+self.nineq,:]\n",
    "        if self.neq!=None:\n",
    "            dz=step[:,self.nx+self.nineq:-self.neq,:]\n",
    "            dy=step[:,-self.neq:,:]\n",
    "        else:\n",
    "            dz=step[:,self.nx+self.nineq:,:]\n",
    "            dy=None\n",
    "        return(dx,ds,dz,dy)\n",
    "\n",
    "    # SOLVE_KKT USING BLOCK ELIMINATION TECHNIQUE\n",
    "    # def solve_kkt(self,rx,rs,rz,ry):\n",
    "    #   b1=torch.cat((rx,rs),dim=1)\n",
    "    #   if ry!=None:\n",
    "    #     b2=torch.cat((rz,ry),dim=1)\n",
    "    #   else:\n",
    "    #     b2=rz\n",
    "    #   A11= self.J[:,:self.nx+self.nineq,:self.nx+self.nineq]\n",
    "    #   A12= self.J[:,:self.nx+self.nineq,self.nx+self.nineq:]\n",
    "    #   A21=  torch.transpose(A12,dim0=2, dim1=1)\n",
    "    #   # self.J_lu,self.J_piv= self.lu_factorize(self.J)\n",
    "    #   # U_A11= torch.cholesky(A11)\n",
    "    #   U_A11,U_A11_piv= A11.lu(pivot=False)\n",
    "    #   # u=torch.cholesky_solve(b1,U_A11)\n",
    "    #   u=torch.lu_solve(b1,U_A11,U_A11_piv)\n",
    "    #   # v=torch.cholesky_solve(A12,U_A11)\n",
    "    #   v=torch.lu_solve(A12,U_A11,U_A11_piv)\n",
    "    #   S_neg=torch.bmm(A21,v)\n",
    "    #   U_S_neg,U_S_neg_piv= S_neg.lu(pivot=False)\n",
    "    #   # w= torch.cholesky_solve(b2,U_S_neg)\n",
    "    #   w= torch.lu_solve(b2,U_S_neg,U_S_neg_piv)\n",
    "    #   # t= torch.cholesky_solve(A21,U_S_neg )\n",
    "    #   t= torch.lu_solve(A21,U_S_neg,U_S_neg_piv )\n",
    "    #   x2= -(w-torch.bmm(t,u))\n",
    "    #   x1= u - torch.bmm(v,x2)\n",
    "    #   dx=x1[:,:self.nx,:]\n",
    "    #   ds=x1[:,self.nx:,:]\n",
    "    #   if ry!=None:\n",
    "    #     dz=x2[:,:-self.neq,:]\n",
    "    #   else:\n",
    "    #     dz=x2\n",
    "    #   if ry!=None:\n",
    "    #     dy=x2[:,-self.neq:,:]\n",
    "    #   else:\n",
    "    #     dy=None\n",
    "    #   return (dx,ds,dz,dy)\n",
    "\n",
    "    def remove_nans(self,dx,ds,dz,dy):\n",
    "        wh= torch.where(dx[:,0,:]!=dx[:,0,:])[0] #find NaN positions\n",
    "        dx[wh,:,:]=torch.zeros((len(wh),dx.size()[1],dx.size()[2]),device=self.device,dtype=self.dtype)\n",
    "        ds[wh,:,:]=torch.zeros((len(wh),ds.size()[1],ds.size()[2]),device=self.device,dtype=self.dtype)\n",
    "        dz[wh,:,:]=torch.zeros((len(wh),dz.size()[1],dz.size()[2]),device=self.device,dtype=self.dtype)\n",
    "        if self.neq!=None:\n",
    "            dy[wh,:,:]=torch.zeros((len(wh),dy.size()[1],dy.size()[2]),device=self.device,dtype=self.dtype)\n",
    "        return dx,ds,dz,dy,wh\n",
    "\n",
    "    def mpc_opt(self,print_warning=True):\n",
    "        count=0\n",
    "        bat=np.array([i for i in range(self.nbatch)])\n",
    "        n_iter=0\n",
    "        # this_problem_not_converged= torch.ones(self.nbatch).type_as(self.Q).view(self.nbatch,1,1)\n",
    "        while (n_iter<=3):\n",
    "            if n_iter>0:\n",
    "                print(n_iter)\n",
    "                print(\"Refining solutions with second round of iterations\")\n",
    "            for i in range(self.max_iter):\n",
    "                if self.neq!=None:\n",
    "                    rx= -(torch.bmm(self.A_T,self.y)+torch.bmm(self.G_T,self.z)+torch.bmm(self.Q,self.x)+self.q.unsqueeze(-1))\n",
    "                else:\n",
    "                    rx= -(torch.bmm(self.G_T,self.z)+torch.bmm(self.Q,self.x)+self.q.unsqueeze(-1))\n",
    "                rs=-self.z\n",
    "                rz=-(torch.bmm(self.G,self.x)+self.s-self.h.unsqueeze(-1))\n",
    "                if self.neq!=None:\n",
    "                    ry=-(torch.bmm(self.A,self.x)-self.b.unsqueeze(-1))\n",
    "                else:\n",
    "                    ry=None\n",
    "                d=self.z/self.s\n",
    "                mu=torch.abs(torch.bmm(torch.transpose(self.s,dim0=2,dim1=1),self.z).sum(1))/self.nineq\n",
    "                pri_resid=torch.abs(rx)\n",
    "                dual_1_resid=torch.abs(rz)\n",
    "                if self.neq!=None:\n",
    "                    dual_2_resid=torch.abs(ry)\n",
    "                    resids=np.array([pri_resid.max(),mu.max(),dual_1_resid.max(),dual_2_resid.max()])\n",
    "                else:\n",
    "                    dual_2_resid=torch.zeros_like(dual_1_resid)\n",
    "                    resids=np.array([pri_resid.max(),mu.max(),dual_1_resid.max()])\n",
    "\n",
    "                #find if any of the problems converged\n",
    "                # if (mu < 1e-6).any() :\n",
    "                #   where= torch.where(mu<1e-6)[0]\n",
    "                #   p_resids=torch.max(pri_resid[where,:,:].view(len(where),-1),dim=1).values\n",
    "                #   d1_resids=torch.max(pri_resid[where,:,:].view(len(where),-1),dim=1).values\n",
    "                #   d2_resids=torch.max(pri_resid[where,:,:].view(len(where),-1),dim=1).values\n",
    "                #   sum_resids=p_resids+d1_resids+d2_resids\n",
    "                #   resids_where= where[torch.where(sum_resids<1e-6)[0] ]\n",
    "                #   compareview = resids_where.repeat(where.shape[0],1).T\n",
    "                #   this_problem_not_converged[where,:,:]=0\n",
    "                try:\n",
    "                    if (resids<1e-6).all():\n",
    "                        # print(\"Early exit at iteration no:\",i)\n",
    "                        return(self.x,self.s,self.z,self.y)\n",
    "                except:\n",
    "                    print(bat[torch.isnan(pri_resid.sum(1)).squeeze(1)])\n",
    "                    raise RuntimeError(\"invalid res\")\n",
    "\n",
    "                #affine step calculation\n",
    "                #get modified Jacobian and its lu factorization\n",
    "                self.J=self.get_lu_J(d)\n",
    "                dx_aff,ds_aff,dz_aff,dy_aff=self.solve_kkt(rx,rs,rz,ry)\n",
    "\n",
    "                #affine step size calculation\n",
    "                alpha = torch.min(self.get_step(self.z, dz_aff),self.get_step(self.s, ds_aff))\n",
    "\n",
    "                #affine updates for s and z\n",
    "                s_aff=self.s+alpha*ds_aff\n",
    "                z_aff=self.z+alpha*dz_aff\n",
    "                mu_aff=torch.abs(torch.bmm(torch.transpose(s_aff,dim0=2,dim1=1),z_aff).sum(1))/self.nineq\n",
    "\n",
    "                #find sigma for centering in the direction of mu\n",
    "                sigma=(mu_aff/mu)**3\n",
    "\n",
    "                #find centering+correction steps\n",
    "                rx=torch.zeros((rx.size()),device=self.device,dtype=self.dtype)\n",
    "                rs=((sigma*mu).unsqueeze(-1).repeat(1,self.nineq,1)-ds_aff*dz_aff)/self.s\n",
    "                rz=torch.zeros((rz.size()),device=self.device,dtype=self.dtype)\n",
    "                if self.neq!=None:\n",
    "                    ry=torch.zeros((ry.size()),device=self.device,dtype=self.dtype)\n",
    "                dx_cor,ds_cor,dz_cor,dy_cor=self.solve_kkt(rx,rs,rz,ry)\n",
    "\n",
    "                dx=dx_aff+dx_cor\n",
    "                ds=ds_aff+ds_cor\n",
    "                dz=dz_aff+dz_cor\n",
    "                if self.neq!=None:\n",
    "                    dy=dy_aff+dy_cor\n",
    "                else:\n",
    "                    dy=None\n",
    "                # find update step size\n",
    "                alpha = torch.min(torch.ones((self.nbatch),device=self.device,dtype=self.dtype).view(self.nbatch,1,1),0.99*torch.min(self.get_step(self.z, dz),self.get_step(self.s, ds)))\n",
    "\n",
    "                #check for early exit\n",
    "                # if torch.isnan(dx).all():\n",
    "                #   return(self.x,self.s,self.z,self.y)\n",
    "\n",
    "                # update\n",
    "                # dx[torch.isnan(dx)]=0\n",
    "                # ds[torch.isnan(ds)]=0\n",
    "                # dz[torch.isnan(dz)]=0\n",
    "                # if self.neq!=None:\n",
    "                #   dy[torch.isnan(dy)]=0\n",
    "                dx,ds,dz,dy,wh= self.remove_nans(dx,ds,dz,dy)\n",
    "                if len(wh)== self.nbatch:\n",
    "                    return(self.x,self.s,self.z,self.y)\n",
    "                # dx[dx!=dx]=0\n",
    "                # ds[ds!=ds]=0\n",
    "                # dz[dz!=dz]=0\n",
    "                # if self.neq!=None:\n",
    "                #   dy[dy!=dy]=0\n",
    "                # dx[torch.where(this_problem_not_converged==0)[0],:,:]=0\n",
    "                # ds[torch.where(this_problem_not_converged==0)[0],:,:]=0\n",
    "                # dz[torch.where(this_problem_not_converged==0)[0],:,:]=0\n",
    "                # if self.neq!=None:\n",
    "                #   dy[torch.where(this_problem_not_converged==0)[0],:,:]=0\n",
    "\n",
    "                self.x+=alpha*dx\n",
    "                self.s+=alpha*ds\n",
    "                self.z+=alpha*dz\n",
    "                if self.neq!=None:\n",
    "                    self.y+=alpha*dy\n",
    "                else:\n",
    "                    self.y=None\n",
    "                # do not update problems that already converged\n",
    "                # self.x+=torch.bmm(alpha*dx, this_problem_not_converged)\n",
    "                # self.s+=torch.bmm(alpha*ds, this_problem_not_converged)\n",
    "                # self.z+=torch.bmm(alpha*dz, this_problem_not_converged)\n",
    "                # if self.neq!=None:\n",
    "                #   self.y+=torch.bmm(alpha*dy, this_problem_not_converged)\n",
    "                # else:\n",
    "                #   self.y=None\n",
    "                # if (this_problem_not_converged==0).all():\n",
    "                    # print(\"All problems converged, exiting at iter \",i)\n",
    "                    # return(self.x,self.s,self.z,self.y)\n",
    "\n",
    "                if(i==self.max_iter-1 and (resids>1e-10).any()) & print_warning==True:\n",
    "                    print(\"mpc exit in iter\",i)\n",
    "                    print(\"no of mu not converged: \",len(mu[mu>1e-10]))\n",
    "                    # print(\"no of primal residual not converged: \",len(pri_resid[pri_resid>1e-10]))\n",
    "                    # print(\"no of dual residual 1 not converged: \",len(dual_1_resid[dual_1_resid>1e-10]))\n",
    "                    # print(\"no of dual residual 2 not converged: \",len(dual_2_resid[dual_2_resid>1e-10]))\n",
    "                    print(\"mpc warning: Residuals not converged, need more itrations\")\n",
    "            if self.refine==False:\n",
    "                return(self.x,self.s,self.z,self.y)\n",
    "            else:\n",
    "                n_iter+=1\n",
    "        return(self.x,self.s,self.z,self.y)\n",
    "\n",
    "    def get_step(self,v,dv):\n",
    "        v=v.squeeze(2)\n",
    "        dv=dv.squeeze(2)\n",
    "        div= -v/dv\n",
    "        ones=torch.ones_like(div)\n",
    "        div=torch.where(torch.isinf(div),ones,div)\n",
    "        div=torch.where(torch.isnan(div),ones,div)\n",
    "        div[dv>0]=max(1.0,div.max())\n",
    "        return (div.min(1)[0]).view(v.size()[0],1,1)\n",
    "        \n",
    "    def get_initial(self,z):\n",
    "        #get step size using line search for initialization\n",
    "        nbatch,_,_=z.size()\n",
    "        dz=torch.ones_like(z)\n",
    "        div= -z/dz\n",
    "        alpha=torch.max(div,dim=1).values.view(nbatch,1,1)+1#0.00001\n",
    "        return alpha.view(nbatch,1,1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
