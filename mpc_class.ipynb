{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mpc_class.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNFPMbgpdK56sDB9cAVI/Ej",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annechris13/Master-Thesis/blob/master/mpc_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nu3TmRPBX1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_spd_matrix\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('error')\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM0APPMvaK5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class mpc():\n",
        "  def __init__(self,max_iter=20):\n",
        "    self.max_iter=20\n",
        "\n",
        "  def solve(self,Q,q,G,h,A,b):\n",
        "    self.Q=Q\n",
        "    self.q=q\n",
        "    self.G=G\n",
        "    self.h=h\n",
        "    self.G_T=torch.transpose(self.G,dim0=2,dim1=1)\n",
        "    self.A=A\n",
        "    self.b=b\n",
        "    self.A_T=torch.transpose(self.A,dim0=2,dim1=1)\n",
        "    self.nbatch, self.nx, self.nineq, self.neq = self.get_sizes()\n",
        "    self.is_Q_pd()\n",
        "    \n",
        "    self.J=self.get_Jacobian()\n",
        "    self.J=self.get_lu_J()\n",
        "    #initial solution\n",
        "    self.x,self.s,self.z,self.y=self.solve_kkt(-q.unsqueeze(-1),\n",
        "                                               torch.zeros(self.nbatch,self.nineq).unsqueeze(-1).type_as(self.Q),\n",
        "                                               self.h.unsqueeze(-1),self.b.unsqueeze(-1))\n",
        "    alpha_p=self.get_initial(-self.z)\n",
        "    alpha_d=self.get_initial(self.z)\n",
        "    self.s=-self.z+alpha_p*(torch.ones(self.z.size()).type_as(self.z))\n",
        "    self.z=self.z+alpha_d*(torch.ones(self.z.size()).type_as(self.z))\n",
        "    #main iterations\n",
        "    start = time.time()\n",
        "    self.x,self.s,self.z,self.y=self.mpc_opt()\n",
        "    op_val=0.5*torch.bmm(torch.transpose(self.x,dim0=2,dim1=1),\n",
        "                         torch.bmm(self.Q,self.x))+torch.bmm(\n",
        "                         torch.transpose(self.q.unsqueeze(-1),dim0=2,dim1=1),self.x)\n",
        "    t = time.time() - start\n",
        "    # print(\"Optimization - time taken:\", t)\n",
        "    return self.x, op_val\n",
        "\n",
        "  def get_sizes(self):\n",
        "    #2 dimensions ==> dimensions are (ninenq,nx), add dimension nbatch at pos 0\n",
        "    if(self.Q.dim()==self.G.dim()==self.A.dim()==2):  \n",
        "      self.Q=self.Q.unsqueeze(0)\n",
        "      self.q=self.q.unsqueeze(0)\n",
        "      self.G=self.G.unsqueeze(0)\n",
        "      self.h=self.h.unsqueeze(0)\n",
        "      if A is not None:\n",
        "        self.A=self.A.unsqueeze(0)\n",
        "        self.b=self.b.unsqueeze(0)\n",
        "    #get sizes\n",
        "    nbatch, nineq, nx = self.G.size()\n",
        "    if self.A is not None:\n",
        "      _,neq,_=self.A.size()\n",
        "    else:\n",
        "      neq=None\n",
        "    return nbatch,nx,nineq,neq\n",
        "  \n",
        "  def is_Q_pd(self):\n",
        "    for i in range(self.nbatch):\n",
        "      e,_=torch.eig(self.Q[i])\n",
        "      if not torch.all(e[:,0]>0): \n",
        "        #not all eigen values are positive ==> raise error\n",
        "        raise RuntimeError(\"Q is not PD\")\n",
        "  \n",
        "  def lu_factorize(self,x):\n",
        "    #do lu factorization of x\n",
        "    #avoid pivoting when possible, i.e when on cuda\n",
        "    data, pivots = x.lu(pivot=not x.is_cuda)\n",
        "    #define pivot matrix manually when on cuda \n",
        "    if x.is_cuda==True:\n",
        "        #pivot matrix doesnt do any pivoting\n",
        "        pivots = torch.arange(1, 1+x.size(1),).unsqueeze(0).repeat(x.size(0), 1).int().cuda()\n",
        "    return (data, pivots)\n",
        "\n",
        "  def get_diag_matrix(self,d):\n",
        "    #return diagonal matrix with diagonal entries d\n",
        "    nBatch, n, _ = d.size()\n",
        "    Diag = torch.zeros(nBatch, n, n).type_as(d)\n",
        "    I = torch.eye(n).repeat(nBatch, 1, 1).type_as(d).bool()\n",
        "    Diag[I] = d.view(-1)\n",
        "    return Diag\n",
        "  \n",
        "  def get_Jacobian(self):\n",
        "    #get the jacobian kkt matrix as concatenation of 4 blocks, B2=transpose(B3)\n",
        "    B1=torch.zeros(self.nbatch,self.nx+self.nineq,self.nx+self.nineq).type_as(self.Q)\n",
        "    B3=torch.zeros(self.nbatch,self.neq+self.nineq,self.nx+self.nineq).type_as(self.Q)\n",
        "    B4=torch.zeros(self.nbatch,self.neq+self.nineq,self.neq+self.nineq).type_as(self.Q)\n",
        "\n",
        "    B1[:,:self.nx,:self.nx]=self.Q\n",
        "    #D here is unit identity matrix (initial case)\n",
        "    self.D=torch.eye(self.nineq).repeat(self.nbatch,1,1).type_as(self.Q)\n",
        "    B1[:,-self.nineq:,-self.nineq:]=self.D\n",
        "\n",
        "    B3[:,:self.nineq,:self.nx]=self.G\n",
        "    B3[:,-self.neq:,:self.nx]=self.A\n",
        "    B3[:,:self.nineq,self.nineq:]=torch.eye(self.nineq).repeat(self.nbatch,1,1).type_as(self.Q)\n",
        "\n",
        "    B2=torch.transpose(B3, dim0=2, dim1=1)\n",
        "    self.J=torch.cat((torch.cat((B1,B2),dim=2),torch.cat((B3,B4),dim=2)),dim=1)\n",
        "    return self.J\n",
        "\n",
        "  def get_lu_J(self,d=None):\n",
        "    # the jacobian J is modified when d is specified\n",
        "    if d!=None:\n",
        "      self.D=self.get_diag_matrix(d)\n",
        "      self.J[:,self.nx:self.nx+self.nineq,self.nx:self.nx+self.nineq]=self.D\n",
        "    self.J_lu,self.J_piv= self.lu_factorize(self.J)\n",
        "    return self.J\n",
        "\n",
        "  def solve_kkt(self,rx,rs,rz,ry):\n",
        "    #TODO: Implement solving the KKT system using block elimination\n",
        "    # solve the KKT system with jacobian J and F specified by rx,rs,rz,ry\n",
        "    F=torch.cat((rx,rs,rz,ry), dim=1)\n",
        "    step=F.lu_solve(self.J_lu,self.J_piv)\n",
        "    dx=step[:,:self.nx,:]\n",
        "    ds=step[:,self.nx:self.nx+self.nineq,:]\n",
        "    dz=step[:,self.nx+self.nineq:-self.neq,:]\n",
        "    dy=step[:,-self.neq:,:]\n",
        "    return(dx,ds,dz,dy)\n",
        "  \n",
        "  def mpc_opt(self):\n",
        "    # J=self.get_Jacobian()\n",
        "    count=0\n",
        "    bat=np.array([i for i in range(self.nbatch)])\n",
        "    for i in range(self.max_iter):\n",
        "        # print(\"iteration: \",i)\n",
        "        rx= -(torch.bmm(self.A_T,self.y)+torch.bmm(self.G_T,self.z)+torch.bmm(self.Q,self.x)+self.q.unsqueeze(-1))\n",
        "        rs=-self.z\n",
        "        rz=-(torch.bmm(self.G,self.x)+self.s-self.h.unsqueeze(-1))\n",
        "        ry=-(torch.bmm(self.A,self.x)-self.b.unsqueeze(-1))\n",
        "        d=self.z/self.s\n",
        "        mu=torch.abs(torch.bmm(torch.transpose(self.s,dim0=2,dim1=1),self.z).sum(1))\n",
        "        pri_resid=torch.abs(rx)\n",
        "        dual_1_resid=torch.abs(rz)\n",
        "        dual_2_resid=torch.abs(ry)\n",
        "        # if (i%4==0):\n",
        "        #   log=((pri_resid.sum(1)<1e-12)*(dual_1_resid.sum(1)<1e-12)*(dual_2_resid.sum(1)<1e-12)*(mu<1e-12)).squeeze(1).numpy().astype(bool)\n",
        "          # print(\"already converged: \",bat[log] )\n",
        "\n",
        "        resids=np.array([pri_resid.max(),mu.max(),dual_1_resid.max(),dual_2_resid.max()])\n",
        "        try:\n",
        "          if (resids<1e-12).all():\n",
        "            # print(\"Early exit at iteration no:\",i)\n",
        "            return(self.x,self.s,self.z,self.y)\n",
        "        except:\n",
        "          print(bat[torch.isnan(pri_resid.sum(1)).squeeze(1)])\n",
        "          raise RuntimeError(\"invalid res\")\n",
        "        \n",
        "        #affine step calculation\n",
        "        #get modified Jacobian and its lu factorization\n",
        "        self.J=self.get_lu_J(d)\n",
        "        dx_aff,ds_aff,dz_aff,dy_aff=self.solve_kkt(rx,rs,rz,ry)\n",
        "        #affine step size calculation\n",
        "        alpha = torch.min(self.get_step(self.z, dz_aff),self.get_step(self.s, ds_aff))\n",
        "        \n",
        "        #affine updates for s and z\n",
        "        s_aff=self.s+alpha*ds_aff\n",
        "        z_aff=self.z+alpha*dz_aff\n",
        "        mu_aff=torch.abs(torch.bmm(torch.transpose(s_aff,dim0=2,dim1=1),z_aff).sum(1))\n",
        "        \n",
        "        #find sigma for centering in the direction of mu\n",
        "        sigma=(mu_aff/mu)**3\n",
        "\n",
        "        #find centering+correction steps\n",
        "        rx=torch.zeros(rx.size()).type_as(self.Q)\n",
        "        rs=((sigma*mu).unsqueeze(-1).repeat(1,self.nineq,1)-ds_aff*dz_aff)/self.s\n",
        "        rz=torch.zeros(rz.size()).type_as(self.Q)\n",
        "        ry=torch.zeros(ry.size()).type_as(self.Q)\n",
        "        dx_cor,ds_cor,dz_cor,dy_cor=self.solve_kkt(rx,rs,rz,ry)\n",
        "\n",
        "        dx=dx_aff+dx_cor\n",
        "        ds=ds_aff+ds_cor\n",
        "        dz=dz_aff+dz_cor\n",
        "        dy=dy_aff+dy_cor\n",
        "        # find update step size\n",
        "        alpha = torch.min(torch.ones(self.nbatch).type_as(self.Q).view(self.nbatch,1,1),0.99*torch.min(self.get_step(self.z, dz),self.get_step(self.s, ds)))\n",
        "        # update\n",
        "        self.x+=alpha*dx\n",
        "        self.s+=alpha*ds\n",
        "        self.z+=alpha*dz\n",
        "        self.y+=alpha*dy\n",
        "\n",
        "        if(i==self.max_iter-1 and (resids>1e-10).any()):\n",
        "          print(\"no of mu not converged: \",len(mu[mu>1e-10]))\n",
        "          # print(\"no of primal residual not converged: \",len(pri_resid[pri_resid>1e-10]))\n",
        "          # print(\"no of dual residual 1 not converged: \",len(dual_1_resid[dual_1_resid>1e-10]))\n",
        "          # print(\"no of dual residual 2 not converged: \",len(dual_2_resid[dual_2_resid>1e-10]))\n",
        "          print(\"mpc warning: Residuals not converged, need more itrations\")\n",
        "    return(self.x,self.s,self.z,self.y)\n",
        "\n",
        "  def get_step(self,v,dv):\n",
        "    v=v.squeeze(2)\n",
        "    dv=dv.squeeze(2)\n",
        "    div= -v/dv\n",
        "    ones=torch.ones_like(div)\n",
        "    div=torch.where(torch.isinf(div),ones,div)\n",
        "    div=torch.where(torch.isnan(div),ones,div)\n",
        "    div[dv>0]=max(1.0,div.max())\n",
        "    return (div.min(1)[0]).view(v.size()[0],1,1)\n",
        "  def get_initial(self,z):\n",
        "      #get step size using line search for initialization \n",
        "      nbatch,_,_=z.size()\n",
        "      dz=torch.ones(z.size()).type_as(z)\n",
        "      alpha=torch.tensor([]).type_as(z)\n",
        "      for b in range(nbatch):\n",
        "        step=torch.tensor([-0.1]).type_as(z)\n",
        "        z_=z[b,:,:]\n",
        "        dz_=dz[b,:,:]\n",
        "        while True:\n",
        "          if (z_+step*dz_ >0).all():\n",
        "            if step<0:\n",
        "              alpha=torch.cat((alpha,torch.tensor([0]).type_as(z)))\n",
        "            else:\n",
        "              alpha=torch.cat((alpha,1+step))\n",
        "            break\n",
        "          else:\n",
        "            step=step+0.1\n",
        "      return alpha.view(nbatch,1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_FMqleAowJ6",
        "colab_type": "text"
      },
      "source": [
        "# Test Problem\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otpfmi9ZlnI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# nb=2\n",
        "# # \n",
        "# xy=2\n",
        "# ineq=2\n",
        "# eq=1\n",
        "# #to do: extract dimensions from problem parameters + check/add batch dimension\n",
        "# Q_=torch.tensor([[4,1,1,2],[6,2,2,2]]).view(nb,xy,xy).type(torch.DoubleTensor)\n",
        "# q_=torch.tensor([[1,1],[1,6]]).view(nb,xy).type(torch.DoubleTensor)\n",
        "# G_=torch.tensor([[-1,0,0,-1],[-1,0,0,-1]]).view(nb,ineq,xy).type(torch.DoubleTensor)\n",
        "# h_=torch.tensor([[0,0],[0,0]]).view(nb,ineq).type(torch.DoubleTensor)\n",
        "# A_=torch.tensor([[1,1],[2,3]]).view(nb,eq,xy).type(torch.DoubleTensor)\n",
        "# b_=torch.tensor([[1],[4]]).view(nb,eq).type(torch.DoubleTensor)\n",
        "# solver=mpc()\n",
        "# solver.solve(Q_,q_,G_,h_,A_,b_)\n",
        "# # # do,po=lu_hack(Q)\n",
        "# # # print(po)\n",
        "# # # print(do)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88jxVW5Ko25t",
        "colab_type": "text"
      },
      "source": [
        "# Non-class Implementation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9m160zXBgwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def check_Q_pd(Q):\n",
        "#   #check if Q is pd:\n",
        "#   nbatch=Q.size()[0]\n",
        "#   for i in range(nbatch):\n",
        "#     e,_=torch.eig(Q[i])\n",
        "#     if not torch.all(e[:,0]>0):\n",
        "#       raise RuntimeError(\"Q is not PD\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0KWqfKaBr5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def lu_hack(x):\n",
        "#     #do lu factorization of x\n",
        "#     data, pivots = x.lu(pivot=not x.is_cuda)\n",
        "#     if x.is_cuda:\n",
        "#         if x.ndimension() == 2:\n",
        "#             pivots = torch.arange(1, 1+x.size(0)).int().cuda()\n",
        "#         elif x.ndimension() == 3:\n",
        "#             pivots = torch.arange(\n",
        "#                 1, 1+x.size(1),\n",
        "#             ).unsqueeze(0).repeat(x.size(0), 1).int().cuda()\n",
        "#         else:\n",
        "#             assert False\n",
        "#     return (data, pivots)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozyl6TS-Bt1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def bdiag(d):\n",
        "#     #return diagonal matrix with diagonal entries d\n",
        "#     nBatch, sz, _ = d.size()\n",
        "#     D = torch.zeros(nBatch, sz, sz).type_as(d)\n",
        "#     I = torch.eye(sz).repeat(nBatch, 1, 1).type_as(d).bool()\n",
        "#     D[I] = d.squeeze().view(-1)\n",
        "#     return D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Co4ppqBv4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_Hessian(Q,G,A):\n",
        "#     #get the hessian kkt matrix\n",
        "#     nbatch,nineq,nx=G.size()\n",
        "#     neq=A.size()[1]\n",
        "#     B1=torch.zeros(nbatch,nx+nineq,nx+nineq).type_as(Q)\n",
        "#     B3=torch.zeros(nbatch,neq+nineq,nx+nineq).type_as(Q)\n",
        "#     B4=torch.zeros(nbatch,neq+nineq,neq+nineq).type_as(Q)\n",
        "\n",
        "#     B1[:,:nx,:nx]=Q\n",
        "#     #D here is unit identity matrix\n",
        "#     B1[:,-nineq:,-nineq:]=torch.eye(nineq).repeat(nbatch,1,1).type_as(Q)\n",
        "\n",
        "#     B3[:,:nineq,:nx]=G\n",
        "#     B3[:,-neq:,:nx]=A\n",
        "#     B3[:,:nineq,nineq:]=torch.eye(nineq).repeat(nbatch,1,1).type_as(Q)\n",
        "\n",
        "#     B2=torch.transpose(B3, dim0=2, dim1=1)\n",
        "\n",
        "#     H=torch.cat((torch.cat((B1,B2),dim=2),torch.cat((B3,B4),dim=2)),dim=1)\n",
        "  \n",
        "#     return H"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Oje_ht_CDMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def solve_kkt(H,rx,rs,rz,ry,d=None):\n",
        "#     # solve the KKT system with hessian H and F specified by rx,rs,rz,ry\n",
        "#     # the hessian H is modified when d is specified\n",
        "#     nx=rx.size()[1]\n",
        "#     nineq=rz.size()[1]\n",
        "#     neq=ry.size()[1]\n",
        "#     if d!=None:\n",
        "#       D=bdiag(d)\n",
        "#       H[:,nx:nx+nineq,nx:nx+nineq]=D\n",
        "#     # print(\"H: \",H)\n",
        "#     H_lu,H_piv= lu_hack(H)\n",
        "#     F=torch.cat((rx,rs,rz,ry), dim=1)\n",
        "#     step=F.lu_solve(H_lu,H_piv)\n",
        "\n",
        "#     rx=step[:,:nx,:]\n",
        "#     rs=step[:,nx:nx+nineq,:]\n",
        "#     rz=step[:,nx+nineq:-neq,:]\n",
        "#     ry=step[:,-neq:,:]\n",
        "#     return(rx,rs,rz,ry)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjlTyY0_CcSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_initial(z):\n",
        "#       #get step size using line search for initialization \n",
        "#       nbatch,_,_=z.size()\n",
        "#       dz=torch.ones(z.size()).type_as(z)\n",
        "#       alpha=torch.tensor([]).type_as(z)\n",
        "#       for b in range(nbatch):\n",
        "#         step=torch.tensor([-0.1]).type_as(z)\n",
        "#         z_=z[b,:,:]\n",
        "#         dz_=dz[b,:,:]\n",
        "#         while True:\n",
        "#           if (z_+step*dz_ >0).all():\n",
        "#             if step<0:\n",
        "#               alpha=torch.cat((alpha,torch.tensor([0]).type_as(z)))\n",
        "#             else:\n",
        "#               alpha=torch.cat((alpha,1+step))\n",
        "#             break\n",
        "#           else:\n",
        "#             step=step+0.1\n",
        "#       return alpha.view(nbatch,1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-wD9wg8Cnug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_step(v,dv):\n",
        "#       #get step sizes for each iteration\n",
        "#       #TO DO: find efficient and accurate line search algorithm\n",
        "#       nbatch,_,_=v.size()\n",
        "#       alpha=torch.tensor([]).type_as(v)\n",
        "#       for b in range(nbatch):\n",
        "#         step=torch.tensor([1]).type_as(v)\n",
        "#         v_=v[b,:,:]\n",
        "#         dv_=dv[b,:,:]\n",
        "#         while step>0:\n",
        "#           if (v_+step*dv_ >=0).all() or step==0:\n",
        "#             alpha=torch.cat((alpha,step))\n",
        "#             break\n",
        "#           else:\n",
        "#             step=step-0.01\n",
        "#         if(step<0):\n",
        "#           alpha=torch.cat((alpha,torch.tensor([0]).type_as(v)))\n",
        "#       return alpha.view(nbatch,1,1)\n",
        "\n",
        "# def get_step(v, dv):\n",
        "#     #qpth version of get_step\n",
        "#     a = -v / dv\n",
        "#     a[dv > 0] = max(1.0, a.max())\n",
        "#     return a.min(1)[0].squeeze()\n",
        "\n",
        "# def get_step(v,dv):\n",
        "#   v=v.squeeze(2)\n",
        "#   dv=dv.squeeze(2)\n",
        "#   div= -v/dv\n",
        "#   ones=torch.ones_like(div)\n",
        "#   div=torch.where(torch.isinf(div),ones,div)\n",
        "#   div=torch.where(torch.isnan(div),ones,div)\n",
        "#   div[dv>0]=max(1.0,div.max())\n",
        "#   return (div.min(1)[0]).view(v.size()[0],1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsRlMRzHEjZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def mpc(Q,G,A,p,b,h,x,s,z,y, max_iter=20):\n",
        "#     nbatch,nx,_=Q.size()\n",
        "#     nineq=G.size()[1]\n",
        "#     neq=A.size()[1]\n",
        "#     H=get_Hessian(Q,G,A)\n",
        "#     A_T=torch.transpose(A,dim0=2,dim1=1)\n",
        "#     G_T=torch.transpose(G,dim0=2,dim1=1)\n",
        "#     count=0\n",
        "#     bat=np.array([i for i in range(nbatch)])\n",
        "#     for i in range(max_iter):\n",
        "#         # print(\"iteration: \",i)\n",
        "#         rx= -(torch.bmm(A_T,y)+torch.bmm(G_T,z)+torch.bmm(Q,x)+p.unsqueeze(2))\n",
        "#         rs=-z\n",
        "#         rz=-(torch.bmm(G,x)+s-h.unsqueeze(2))\n",
        "#         ry=-(torch.bmm(A,x)-b.unsqueeze(2))\n",
        "#         d=z/s\n",
        "#         mu=torch.abs(torch.bmm(torch.transpose(s,dim0=2,dim1=1),z).sum(1))\n",
        "#         pri_resid=torch.abs(rx)\n",
        "#         dual_1_resid=torch.abs(rz)\n",
        "#         dual_2_resid=torch.abs(ry)\n",
        "#         if (i%4==0):\n",
        "#           log=((pri_resid.sum(1)<1e-12)*(dual_1_resid.sum(1)<1e-12)*(dual_2_resid.sum(1)<1e-12)*(mu<1e-12)).squeeze(1).numpy().astype(bool)\n",
        "#           # print(\"already converged: \",bat[log] )\n",
        "\n",
        "#         resids=np.array([pri_resid.max(),mu.max(),dual_1_resid.max(),dual_2_resid.max()])\n",
        "#         try:\n",
        "#           if (resids<1e-12).all():\n",
        "#             # print(\"Early exit at iteration no:\",i)\n",
        "#             return(x,s,z,y)\n",
        "#         except:\n",
        "#           print(bat[torch.isnan(pri_resid.sum(1)).squeeze(1)])\n",
        "#           raise RuntimeError(\"invalid res\")\n",
        "        \n",
        "#         #affine step calculation\n",
        "#         dx_aff,ds_aff,dz_aff,dy_aff=solve_kkt(H,rx,rs,rz,ry,d)\n",
        "#         #affine step size calculation\n",
        "#         alpha = torch.min(get_step(z, dz_aff),get_step(s, ds_aff))\n",
        "        \n",
        "#         #affine updates for s and z\n",
        "#         s_aff=s+alpha*ds_aff\n",
        "#         z_aff=z+alpha*dz_aff\n",
        "#         mu_aff=torch.abs(torch.bmm(torch.transpose(s_aff,dim0=2,dim1=1),z_aff).sum(1))\n",
        "        \n",
        "#         #find sigma for centering in the direction of mu\n",
        "#         sigma=(mu_aff/mu)**3\n",
        "\n",
        "#         #find centering+correction steps\n",
        "#         rx=torch.zeros(rx.size()).type_as(Q)\n",
        "#         rs=((sigma*mu).unsqueeze(2).repeat(1,nineq,1)-ds_aff*dz_aff)/s\n",
        "#         rz=torch.zeros(rz.size()).type_as(Q)\n",
        "#         ry=torch.zeros(ry.size()).type_as(Q)\n",
        "#         dx_cor,ds_cor,dz_cor,dy_cor=solve_kkt(H,rx,rs,rz,ry,d)\n",
        "\n",
        "#         dx=dx_aff+dx_cor\n",
        "#         ds=ds_aff+ds_cor\n",
        "#         dz=dz_aff+dz_cor\n",
        "#         dy=dy_aff+dy_cor\n",
        "#         # find update step size\n",
        "#         alpha = torch.min(torch.ones(nbatch).type_as(Q).view(nbatch,1,1),0.99*torch.min(get_step(z, dz),get_step(s, ds)))\n",
        "#         # update\n",
        "#         x+=alpha*dx\n",
        "#         s+=alpha*ds\n",
        "#         z+=alpha*dz\n",
        "#         y+=alpha*dy\n",
        "\n",
        "#         if(i==max_iter-1 and (resids>1e-10).any()):\n",
        "#           # print(\"no of mu not converged: \",len(mu[mu>1e-10]))\n",
        "#           # print(\"no of primal residual not converged: \",len(pri_resid[pri_resid>1e-10]))\n",
        "#           # print(\"no of dual residual 1 not converged: \",len(dual_1_resid[dual_1_resid>1e-10]))\n",
        "#           # print(\"no of dual residual 2 not converged: \",len(dual_2_resid[dual_2_resid>1e-10]))\n",
        "#           print(\"mpc warning: Residuals not converged, need more itrations\")\n",
        "\n",
        "#     return(x,s,z,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozDfSFHPEkSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def opt(Q,p,G,h,A,b):\n",
        "#   nbatch,nx,_=Q.size()\n",
        "#   nineq=G.size()[1]\n",
        "#   neq=A.size()[1]\n",
        "#   check_Q_pd(Q)\n",
        "#   H=get_Hessian(Q,G,A)\n",
        "#   A_T=torch.transpose(A,dim0=2,dim1=1)\n",
        "#   G_T=torch.transpose(G,dim0=2,dim1=1)\n",
        "#   #initial solution\n",
        "#   x,s,z,y=solve_kkt(H,-p.unsqueeze(2),torch.zeros(nbatch,nineq).unsqueeze(2).type_as(Q),h.unsqueeze(2),b.unsqueeze(2))\n",
        "#   alpha_p=get_initial(-z)\n",
        "#   alpha_d=get_initial(z)\n",
        "#   s=-z+alpha_p*(torch.ones(z.size()).type_as(z))\n",
        "#   z=z+alpha_d*(torch.ones(z.size()).type_as(z))\n",
        "#   #main iterations\n",
        "#   start = time.time()\n",
        "#   x,s,z,y=mpc(Q,G,A,p,b,h,x,s,z,y,30)\n",
        "#   op_val=0.5*torch.bmm(torch.transpose(x,dim0=2,dim1=1),torch.bmm(Q,x))+torch.bmm(torch.transpose(p.unsqueeze(2),dim0=2,dim1=1),x)\n",
        "#   t = time.time() - start\n",
        "#   # print(t)\n",
        "#   return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZRN4j2aEpDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}