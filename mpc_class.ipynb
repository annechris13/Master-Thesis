{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/annechris13/Master-Thesis/blob/master/mpc_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6nu3TmRPBX1b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LM0APPMvaK5O"
   },
   "outputs": [],
   "source": [
    "class mpc():\n",
    "  def __init__(self,max_iter=20):\n",
    "    self.max_iter=max_iter\n",
    "\n",
    "  def solve(self,Q,q,G,h,A,b):\n",
    "    self.Q=Q\n",
    "    self.q=q\n",
    "    self.G=G\n",
    "    self.h=h\n",
    "    self.G_T=torch.transpose(self.G,dim0=2,dim1=1)\n",
    "    self.A=A\n",
    "    self.b=b\n",
    "    self.A_T=torch.transpose(self.A,dim0=2,dim1=1)\n",
    "    self.nbatch, self.nx, self.nineq, self.neq = self.get_sizes()\n",
    "    self.is_Q_pd()\n",
    "    \n",
    "    self.J=self.get_Jacobian()\n",
    "    self.J=self.get_lu_J()\n",
    "    #initial solution\n",
    "    self.x,self.s,self.z,self.y=self.solve_kkt(-q.unsqueeze(-1),\n",
    "                                               torch.zeros(self.nbatch,self.nineq).unsqueeze(-1).type_as(self.Q),\n",
    "                                               self.h.unsqueeze(-1),self.b.unsqueeze(-1))\n",
    "    alpha_p=self.get_initial(-self.z)\n",
    "    alpha_d=self.get_initial(self.z)\n",
    "    self.s=-self.z+alpha_p*(torch.ones(self.z.size()).type_as(self.z))\n",
    "    self.z=self.z+alpha_d*(torch.ones(self.z.size()).type_as(self.z))\n",
    "    #main iterations\n",
    "    start = time.time()\n",
    "    self.x,self.s,self.z,self.y=self.mpc_opt()\n",
    "    op_val=0.5*torch.bmm(torch.transpose(self.x,dim0=2,dim1=1),\n",
    "                         torch.bmm(self.Q,self.x))+torch.bmm(\n",
    "                         torch.transpose(self.q.unsqueeze(-1),dim0=2,dim1=1),self.x)\n",
    "    t = time.time() - start\n",
    "    # print(\"Optimization - time taken:\", t)\n",
    "    return self.x, op_val\n",
    "\n",
    "  def get_sizes(self):\n",
    "    #2 dimensions ==> dimensions are (ninenq,nx), add dimension nbatch at pos 0\n",
    "    if(self.Q.dim()==self.G.dim()==self.A.dim()==2):  \n",
    "      self.Q=self.Q.unsqueeze(0)\n",
    "      self.q=self.q.unsqueeze(0)\n",
    "      self.G=self.G.unsqueeze(0)\n",
    "      self.h=self.h.unsqueeze(0)\n",
    "      if A is not None:\n",
    "        self.A=self.A.unsqueeze(0)\n",
    "        self.b=self.b.unsqueeze(0)\n",
    "    #get sizes\n",
    "    nbatch, nineq, nx = self.G.size()\n",
    "    if self.A is not None:\n",
    "      _,neq,_=self.A.size()\n",
    "    else:\n",
    "      neq=None\n",
    "    return nbatch,nx,nineq,neq\n",
    "  \n",
    "  def is_Q_pd(self):\n",
    "    try:\n",
    "        # np.linalg\n",
    "        torch.cholesky(self.Q)\n",
    "    except:\n",
    "        raise RuntimeError(\"Q is not PD\")\n",
    "#     for i in range(self.nbatch):\n",
    "#       e,_=torch.eig(self.Q[i])\n",
    "#       if not torch.all(e[:,0]>0): \n",
    "#         #not all eigen values are positive ==> raise error\n",
    "#         raise RuntimeError(\"Q is not PD\")\n",
    "  \n",
    "  def lu_factorize(self,x):\n",
    "    #do lu factorization of x\n",
    "    #avoid pivoting when possible, i.e when on cuda\n",
    "    data, pivots = x.lu(pivot=not x.is_cuda)\n",
    "    #define pivot matrix manually when on cuda \n",
    "    if x.is_cuda==True:\n",
    "        #pivot matrix doesnt do any pivoting\n",
    "        pivots = torch.arange(1, 1+x.size(1),).unsqueeze(0).repeat(x.size(0), 1).int().cuda()\n",
    "    return (data, pivots)\n",
    "\n",
    "  def get_diag_matrix(self,d):\n",
    "    #return diagonal matrix with diagonal entries d\n",
    "    nBatch, n, _ = d.size()\n",
    "    Diag = torch.zeros(nBatch, n, n).type_as(d)\n",
    "    I = torch.eye(n).repeat(nBatch, 1, 1).type_as(d).bool()\n",
    "    Diag[I] = d.view(-1)\n",
    "    return Diag\n",
    "  \n",
    "  def get_Jacobian(self):\n",
    "    #get the jacobian kkt matrix as concatenation of 4 blocks, B2=transpose(B3)\n",
    "    B1=torch.zeros(self.nbatch,self.nx+self.nineq,self.nx+self.nineq).type_as(self.Q)\n",
    "    B3=torch.zeros(self.nbatch,self.neq+self.nineq,self.nx+self.nineq).type_as(self.Q)\n",
    "    B4=torch.zeros(self.nbatch,self.neq+self.nineq,self.neq+self.nineq).type_as(self.Q)\n",
    "\n",
    "    B1[:,:self.nx,:self.nx]=self.Q\n",
    "    #D here is unit identity matrix (initial case)\n",
    "    self.D=torch.eye(self.nineq).repeat(self.nbatch,1,1).type_as(self.Q)\n",
    "    B1[:,-self.nineq:,-self.nineq:]=self.D\n",
    "\n",
    "    B3[:,:self.nineq,:self.nx]=self.G\n",
    "    B3[:,-self.neq:,:self.nx]=self.A\n",
    "    B3[:,:self.nineq,self.nineq:]=torch.eye(self.nineq).repeat(self.nbatch,1,1).type_as(self.Q)\n",
    "\n",
    "    B2=torch.transpose(B3, dim0=2, dim1=1)\n",
    "    self.J=torch.cat((torch.cat((B1,B2),dim=2),torch.cat((B3,B4),dim=2)),dim=1)\n",
    "    return self.J\n",
    "\n",
    "  def get_lu_J(self,d=None):\n",
    "    # the jacobian J is modified when d is specified\n",
    "    if d!=None:\n",
    "      self.D=self.get_diag_matrix(d)\n",
    "      self.J[:,self.nx:self.nx+self.nineq,self.nx:self.nx+self.nineq]=self.D\n",
    "    self.J_lu,self.J_piv= self.lu_factorize(self.J)\n",
    "    return self.J\n",
    "\n",
    "  def solve_kkt(self,rx,rs,rz,ry):\n",
    "    #TODO: Implement solving the KKT system using block elimination\n",
    "    # solve the KKT system with jacobian J and F specified by rx,rs,rz,ry\n",
    "    F=torch.cat((rx,rs,rz,ry), dim=1)\n",
    "    step=F.lu_solve(self.J_lu,self.J_piv)\n",
    "    dx=step[:,:self.nx,:]\n",
    "    ds=step[:,self.nx:self.nx+self.nineq,:]\n",
    "    dz=step[:,self.nx+self.nineq:-self.neq,:]\n",
    "    dy=step[:,-self.neq:,:]\n",
    "    return(dx,ds,dz,dy)\n",
    "  \n",
    "  def mpc_opt(self):\n",
    "    # J=self.get_Jacobian()\n",
    "    count=0\n",
    "    bat=np.array([i for i in range(self.nbatch)])\n",
    "    for i in range(self.max_iter):\n",
    "        # print(\"iteration: \",i)\n",
    "        rx= -(torch.bmm(self.A_T,self.y)+torch.bmm(self.G_T,self.z)+torch.bmm(self.Q,self.x)+self.q.unsqueeze(-1))\n",
    "        rs=-self.z\n",
    "        rz=-(torch.bmm(self.G,self.x)+self.s-self.h.unsqueeze(-1))\n",
    "        ry=-(torch.bmm(self.A,self.x)-self.b.unsqueeze(-1))\n",
    "        d=self.z/self.s\n",
    "        mu=torch.abs(torch.bmm(torch.transpose(self.s,dim0=2,dim1=1),self.z).sum(1))\n",
    "        pri_resid=torch.abs(rx)\n",
    "        dual_1_resid=torch.abs(rz)\n",
    "        dual_2_resid=torch.abs(ry)\n",
    "        # if (i%4==0):\n",
    "        #   log=((pri_resid.sum(1)<1e-12)*(dual_1_resid.sum(1)<1e-12)*(dual_2_resid.sum(1)<1e-12)*(mu<1e-12)).squeeze(1).numpy().astype(bool)\n",
    "          # print(\"already converged: \",bat[log] )\n",
    "\n",
    "        resids=np.array([pri_resid.max(),mu.max(),dual_1_resid.max(),dual_2_resid.max()])\n",
    "        try:\n",
    "          if (resids<1e-12).all():\n",
    "            # print(\"Early exit at iteration no:\",i)\n",
    "            return(self.x,self.s,self.z,self.y)\n",
    "        except:\n",
    "          print(bat[torch.isnan(pri_resid.sum(1)).squeeze(1)])\n",
    "          raise RuntimeError(\"invalid res\")\n",
    "        \n",
    "        #affine step calculation\n",
    "        #get modified Jacobian and its lu factorization\n",
    "        self.J=self.get_lu_J(d)\n",
    "        dx_aff,ds_aff,dz_aff,dy_aff=self.solve_kkt(rx,rs,rz,ry)\n",
    "        #affine step size calculation\n",
    "        alpha = torch.min(self.get_step(self.z, dz_aff),self.get_step(self.s, ds_aff))\n",
    "        \n",
    "        #affine updates for s and z\n",
    "        s_aff=self.s+alpha*ds_aff\n",
    "        z_aff=self.z+alpha*dz_aff\n",
    "        mu_aff=torch.abs(torch.bmm(torch.transpose(s_aff,dim0=2,dim1=1),z_aff).sum(1))\n",
    "        \n",
    "        #find sigma for centering in the direction of mu\n",
    "        sigma=(mu_aff/mu)**3\n",
    "\n",
    "        #find centering+correction steps\n",
    "        rx=torch.zeros(rx.size()).type_as(self.Q)\n",
    "        rs=((sigma*mu).unsqueeze(-1).repeat(1,self.nineq,1)-ds_aff*dz_aff)/self.s\n",
    "        rz=torch.zeros(rz.size()).type_as(self.Q)\n",
    "        ry=torch.zeros(ry.size()).type_as(self.Q)\n",
    "        dx_cor,ds_cor,dz_cor,dy_cor=self.solve_kkt(rx,rs,rz,ry)\n",
    "\n",
    "        dx=dx_aff+dx_cor\n",
    "        ds=ds_aff+ds_cor\n",
    "        dz=dz_aff+dz_cor\n",
    "        dy=dy_aff+dy_cor\n",
    "        # find update step size\n",
    "        alpha = torch.min(torch.ones(self.nbatch).type_as(self.Q).view(self.nbatch,1,1),0.99*torch.min(self.get_step(self.z, dz),self.get_step(self.s, ds)))\n",
    "        # update\n",
    "        self.x+=alpha*dx\n",
    "        self.s+=alpha*ds\n",
    "        self.z+=alpha*dz\n",
    "        self.y+=alpha*dy\n",
    "\n",
    "        if(i==self.max_iter-1 and (resids>1e-10).any()):\n",
    "          print(\"no of mu not converged: \",len(mu[mu>1e-10]))\n",
    "          # print(\"no of primal residual not converged: \",len(pri_resid[pri_resid>1e-10]))\n",
    "          # print(\"no of dual residual 1 not converged: \",len(dual_1_resid[dual_1_resid>1e-10]))\n",
    "          # print(\"no of dual residual 2 not converged: \",len(dual_2_resid[dual_2_resid>1e-10]))\n",
    "          print(\"mpc warning: Residuals not converged, need more itrations\")\n",
    "    return(self.x,self.s,self.z,self.y)\n",
    "\n",
    "  def get_step(self,v,dv):\n",
    "    v=v.squeeze(2)\n",
    "    dv=dv.squeeze(2)\n",
    "    div= -v/dv\n",
    "    ones=torch.ones_like(div)\n",
    "    div=torch.where(torch.isinf(div),ones,div)\n",
    "    div=torch.where(torch.isnan(div),ones,div)\n",
    "    div[dv>0]=max(1.0,div.max())\n",
    "    return (div.min(1)[0]).view(v.size()[0],1,1)\n",
    "  def get_initial(self,z):\n",
    "      #get step size using line search for initialization \n",
    "      nbatch,_,_=z.size()\n",
    "      dz=torch.ones(z.size()).type_as(z)\n",
    "      div= -z/dz\n",
    "      alpha=torch.max(div,dim=1).values.view(nbatch,1,1)+1#0.00001\n",
    "      return alpha.view(nbatch,1,1)\n",
    "#   def get_initial(self,z):\n",
    "#       #get step size using line search for initialization \n",
    "#       nbatch,_,_=z.size()\n",
    "#       dz=torch.ones(z.size()).type_as(z)\n",
    "#       alpha=torch.tensor([]).type_as(z)\n",
    "#       for b in range(nbatch):\n",
    "#         step=torch.tensor([-0.1]).type_as(z)\n",
    "#         z_=z[b,:,:]\n",
    "#         dz_=dz[b,:,:]\n",
    "#         while True:\n",
    "#           if (z_+step*dz_ >0).all():\n",
    "#             if step<0:\n",
    "#               alpha=torch.cat((alpha,torch.tensor([0]).type_as(z)))\n",
    "#             else:\n",
    "#               alpha=torch.cat((alpha,1+step))\n",
    "#             break\n",
    "#           else:\n",
    "#             step=step+0.1\n",
    "#       return alpha.view(nbatch,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_FMqleAowJ6"
   },
   "source": [
    "# Test Problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "otpfmi9ZlnI0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# nb=2\n",
    "# # \n",
    "# xy=2\n",
    "# ineq=2\n",
    "# eq=1\n",
    "# #to do: extract dimensions from problem parameters + check/add batch dimension\n",
    "# Q_=torch.tensor([[4,1,1,2],[6,2,2,2]]).view(nb,xy,xy).type(torch.DoubleTensor)\n",
    "# q_=torch.tensor([[1,1],[1,6]]).view(nb,xy).type(torch.DoubleTensor)\n",
    "# G_=torch.tensor([[-1,0,0,-1],[-1,0,0,-1]]).view(nb,ineq,xy).type(torch.DoubleTensor)\n",
    "# h_=torch.tensor([[0,0],[0,0]]).view(nb,ineq).type(torch.DoubleTensor)\n",
    "# A_=torch.tensor([[1,1],[2,3]]).view(nb,eq,xy).type(torch.DoubleTensor)\n",
    "# b_=torch.tensor([[1],[4]]).view(nb,eq).type(torch.DoubleTensor)\n",
    "# solver=mpc()\n",
    "# solver.solve(Q_,q_,G_,h_,A_,b_)\n",
    "# # # do,po=lu_hack(Q)\n",
    "# # # print(po)\n",
    "# # # print(do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88jxVW5Ko25t"
   },
   "source": [
    "# Non-class Implementation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "V9m160zXBgwd"
   },
   "outputs": [],
   "source": [
    "# def check_Q_pd(Q):\n",
    "#   #check if Q is pd:\n",
    "#   nbatch=Q.size()[0]\n",
    "#   for i in range(nbatch):\n",
    "#     e,_=torch.eig(Q[i])\n",
    "#     if not torch.all(e[:,0]>0):\n",
    "#       raise RuntimeError(\"Q is not PD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "I0KWqfKaBr5F"
   },
   "outputs": [],
   "source": [
    "# def lu_hack(x):\n",
    "#     #do lu factorization of x\n",
    "#     data, pivots = x.lu(pivot=not x.is_cuda)\n",
    "#     if x.is_cuda:\n",
    "#         if x.ndimension() == 2:\n",
    "#             pivots = torch.arange(1, 1+x.size(0)).int().cuda()\n",
    "#         elif x.ndimension() == 3:\n",
    "#             pivots = torch.arange(\n",
    "#                 1, 1+x.size(1),\n",
    "#             ).unsqueeze(0).repeat(x.size(0), 1).int().cuda()\n",
    "#         else:\n",
    "#             assert False\n",
    "#     return (data, pivots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ozyl6TS-Bt1P"
   },
   "outputs": [],
   "source": [
    "# def bdiag(d):\n",
    "#     #return diagonal matrix with diagonal entries d\n",
    "#     nBatch, sz, _ = d.size()\n",
    "#     D = torch.zeros(nBatch, sz, sz).type_as(d)\n",
    "#     I = torch.eye(sz).repeat(nBatch, 1, 1).type_as(d).bool()\n",
    "#     D[I] = d.squeeze().view(-1)\n",
    "#     return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "R4Co4ppqBv4w"
   },
   "outputs": [],
   "source": [
    "# def get_Hessian(Q,G,A):\n",
    "#     #get the hessian kkt matrix\n",
    "#     nbatch,nineq,nx=G.size()\n",
    "#     neq=A.size()[1]\n",
    "#     B1=torch.zeros(nbatch,nx+nineq,nx+nineq).type_as(Q)\n",
    "#     B3=torch.zeros(nbatch,neq+nineq,nx+nineq).type_as(Q)\n",
    "#     B4=torch.zeros(nbatch,neq+nineq,neq+nineq).type_as(Q)\n",
    "\n",
    "#     B1[:,:nx,:nx]=Q\n",
    "#     #D here is unit identity matrix\n",
    "#     B1[:,-nineq:,-nineq:]=torch.eye(nineq).repeat(nbatch,1,1).type_as(Q)\n",
    "\n",
    "#     B3[:,:nineq,:nx]=G\n",
    "#     B3[:,-neq:,:nx]=A\n",
    "#     B3[:,:nineq,nineq:]=torch.eye(nineq).repeat(nbatch,1,1).type_as(Q)\n",
    "\n",
    "#     B2=torch.transpose(B3, dim0=2, dim1=1)\n",
    "\n",
    "#     H=torch.cat((torch.cat((B1,B2),dim=2),torch.cat((B3,B4),dim=2)),dim=1)\n",
    "  \n",
    "#     return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_Oje_ht_CDMZ"
   },
   "outputs": [],
   "source": [
    "# def solve_kkt(H,rx,rs,rz,ry,d=None):\n",
    "#     # solve the KKT system with hessian H and F specified by rx,rs,rz,ry\n",
    "#     # the hessian H is modified when d is specified\n",
    "#     nx=rx.size()[1]\n",
    "#     nineq=rz.size()[1]\n",
    "#     neq=ry.size()[1]\n",
    "#     if d!=None:\n",
    "#       D=bdiag(d)\n",
    "#       H[:,nx:nx+nineq,nx:nx+nineq]=D\n",
    "#     # print(\"H: \",H)\n",
    "#     H_lu,H_piv= lu_hack(H)\n",
    "#     F=torch.cat((rx,rs,rz,ry), dim=1)\n",
    "#     step=F.lu_solve(H_lu,H_piv)\n",
    "\n",
    "#     rx=step[:,:nx,:]\n",
    "#     rs=step[:,nx:nx+nineq,:]\n",
    "#     rz=step[:,nx+nineq:-neq,:]\n",
    "#     ry=step[:,-neq:,:]\n",
    "#     return(rx,rs,rz,ry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QjlTyY0_CcSi"
   },
   "outputs": [],
   "source": [
    "# def get_initial(z):\n",
    "#       #get step size using line search for initialization \n",
    "#       nbatch,_,_=z.size()\n",
    "#       dz=torch.ones(z.size()).type_as(z)\n",
    "#       alpha=torch.tensor([]).type_as(z)\n",
    "#       for b in range(nbatch):\n",
    "#         step=torch.tensor([-0.1]).type_as(z)\n",
    "#         z_=z[b,:,:]\n",
    "#         dz_=dz[b,:,:]\n",
    "#         while True:\n",
    "#           if (z_+step*dz_ >0).all():\n",
    "#             if step<0:\n",
    "#               alpha=torch.cat((alpha,torch.tensor([0]).type_as(z)))\n",
    "#             else:\n",
    "#               alpha=torch.cat((alpha,1+step))\n",
    "#             break\n",
    "#           else:\n",
    "#             step=step+0.1\n",
    "#       return alpha.view(nbatch,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2-wD9wg8Cnug"
   },
   "outputs": [],
   "source": [
    "# def get_step(v,dv):\n",
    "#       #get step sizes for each iteration\n",
    "#       #TO DO: find efficient and accurate line search algorithm\n",
    "#       nbatch,_,_=v.size()\n",
    "#       alpha=torch.tensor([]).type_as(v)\n",
    "#       for b in range(nbatch):\n",
    "#         step=torch.tensor([1]).type_as(v)\n",
    "#         v_=v[b,:,:]\n",
    "#         dv_=dv[b,:,:]\n",
    "#         while step>0:\n",
    "#           if (v_+step*dv_ >=0).all() or step==0:\n",
    "#             alpha=torch.cat((alpha,step))\n",
    "#             break\n",
    "#           else:\n",
    "#             step=step-0.01\n",
    "#         if(step<0):\n",
    "#           alpha=torch.cat((alpha,torch.tensor([0]).type_as(v)))\n",
    "#       return alpha.view(nbatch,1,1)\n",
    "\n",
    "# def get_step(v, dv):\n",
    "#     #qpth version of get_step\n",
    "#     a = -v / dv\n",
    "#     a[dv > 0] = max(1.0, a.max())\n",
    "#     return a.min(1)[0].squeeze()\n",
    "\n",
    "# def get_step(v,dv):\n",
    "#   v=v.squeeze(2)\n",
    "#   dv=dv.squeeze(2)\n",
    "#   div= -v/dv\n",
    "#   ones=torch.ones_like(div)\n",
    "#   div=torch.where(torch.isinf(div),ones,div)\n",
    "#   div=torch.where(torch.isnan(div),ones,div)\n",
    "#   div[dv>0]=max(1.0,div.max())\n",
    "#   return (div.min(1)[0]).view(v.size()[0],1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VsRlMRzHEjZk"
   },
   "outputs": [],
   "source": [
    "# def mpc(Q,G,A,p,b,h,x,s,z,y, max_iter=20):\n",
    "#     nbatch,nx,_=Q.size()\n",
    "#     nineq=G.size()[1]\n",
    "#     neq=A.size()[1]\n",
    "#     H=get_Hessian(Q,G,A)\n",
    "#     A_T=torch.transpose(A,dim0=2,dim1=1)\n",
    "#     G_T=torch.transpose(G,dim0=2,dim1=1)\n",
    "#     count=0\n",
    "#     bat=np.array([i for i in range(nbatch)])\n",
    "#     for i in range(max_iter):\n",
    "#         # print(\"iteration: \",i)\n",
    "#         rx= -(torch.bmm(A_T,y)+torch.bmm(G_T,z)+torch.bmm(Q,x)+p.unsqueeze(2))\n",
    "#         rs=-z\n",
    "#         rz=-(torch.bmm(G,x)+s-h.unsqueeze(2))\n",
    "#         ry=-(torch.bmm(A,x)-b.unsqueeze(2))\n",
    "#         d=z/s\n",
    "#         mu=torch.abs(torch.bmm(torch.transpose(s,dim0=2,dim1=1),z).sum(1))\n",
    "#         pri_resid=torch.abs(rx)\n",
    "#         dual_1_resid=torch.abs(rz)\n",
    "#         dual_2_resid=torch.abs(ry)\n",
    "#         if (i%4==0):\n",
    "#           log=((pri_resid.sum(1)<1e-12)*(dual_1_resid.sum(1)<1e-12)*(dual_2_resid.sum(1)<1e-12)*(mu<1e-12)).squeeze(1).numpy().astype(bool)\n",
    "#           # print(\"already converged: \",bat[log] )\n",
    "\n",
    "#         resids=np.array([pri_resid.max(),mu.max(),dual_1_resid.max(),dual_2_resid.max()])\n",
    "#         try:\n",
    "#           if (resids<1e-12).all():\n",
    "#             # print(\"Early exit at iteration no:\",i)\n",
    "#             return(x,s,z,y)\n",
    "#         except:\n",
    "#           print(bat[torch.isnan(pri_resid.sum(1)).squeeze(1)])\n",
    "#           raise RuntimeError(\"invalid res\")\n",
    "        \n",
    "#         #affine step calculation\n",
    "#         dx_aff,ds_aff,dz_aff,dy_aff=solve_kkt(H,rx,rs,rz,ry,d)\n",
    "#         #affine step size calculation\n",
    "#         alpha = torch.min(get_step(z, dz_aff),get_step(s, ds_aff))\n",
    "        \n",
    "#         #affine updates for s and z\n",
    "#         s_aff=s+alpha*ds_aff\n",
    "#         z_aff=z+alpha*dz_aff\n",
    "#         mu_aff=torch.abs(torch.bmm(torch.transpose(s_aff,dim0=2,dim1=1),z_aff).sum(1))\n",
    "        \n",
    "#         #find sigma for centering in the direction of mu\n",
    "#         sigma=(mu_aff/mu)**3\n",
    "\n",
    "#         #find centering+correction steps\n",
    "#         rx=torch.zeros(rx.size()).type_as(Q)\n",
    "#         rs=((sigma*mu).unsqueeze(2).repeat(1,nineq,1)-ds_aff*dz_aff)/s\n",
    "#         rz=torch.zeros(rz.size()).type_as(Q)\n",
    "#         ry=torch.zeros(ry.size()).type_as(Q)\n",
    "#         dx_cor,ds_cor,dz_cor,dy_cor=solve_kkt(H,rx,rs,rz,ry,d)\n",
    "\n",
    "#         dx=dx_aff+dx_cor\n",
    "#         ds=ds_aff+ds_cor\n",
    "#         dz=dz_aff+dz_cor\n",
    "#         dy=dy_aff+dy_cor\n",
    "#         # find update step size\n",
    "#         alpha = torch.min(torch.ones(nbatch).type_as(Q).view(nbatch,1,1),0.99*torch.min(get_step(z, dz),get_step(s, ds)))\n",
    "#         # update\n",
    "#         x+=alpha*dx\n",
    "#         s+=alpha*ds\n",
    "#         z+=alpha*dz\n",
    "#         y+=alpha*dy\n",
    "\n",
    "#         if(i==max_iter-1 and (resids>1e-10).any()):\n",
    "#           # print(\"no of mu not converged: \",len(mu[mu>1e-10]))\n",
    "#           # print(\"no of primal residual not converged: \",len(pri_resid[pri_resid>1e-10]))\n",
    "#           # print(\"no of dual residual 1 not converged: \",len(dual_1_resid[dual_1_resid>1e-10]))\n",
    "#           # print(\"no of dual residual 2 not converged: \",len(dual_2_resid[dual_2_resid>1e-10]))\n",
    "#           print(\"mpc warning: Residuals not converged, need more itrations\")\n",
    "\n",
    "#     return(x,s,z,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ozDfSFHPEkSF"
   },
   "outputs": [],
   "source": [
    "# def opt(Q,p,G,h,A,b):\n",
    "#   nbatch,nx,_=Q.size()\n",
    "#   nineq=G.size()[1]\n",
    "#   neq=A.size()[1]\n",
    "#   check_Q_pd(Q)\n",
    "#   H=get_Hessian(Q,G,A)\n",
    "#   A_T=torch.transpose(A,dim0=2,dim1=1)\n",
    "#   G_T=torch.transpose(G,dim0=2,dim1=1)\n",
    "#   #initial solution\n",
    "#   x,s,z,y=solve_kkt(H,-p.unsqueeze(2),torch.zeros(nbatch,nineq).unsqueeze(2).type_as(Q),h.unsqueeze(2),b.unsqueeze(2))\n",
    "#   alpha_p=get_initial(-z)\n",
    "#   alpha_d=get_initial(z)\n",
    "#   s=-z+alpha_p*(torch.ones(z.size()).type_as(z))\n",
    "#   z=z+alpha_d*(torch.ones(z.size()).type_as(z))\n",
    "#   #main iterations\n",
    "#   start = time.time()\n",
    "#   x,s,z,y=mpc(Q,G,A,p,b,h,x,s,z,y,30)\n",
    "#   op_val=0.5*torch.bmm(torch.transpose(x,dim0=2,dim1=1),torch.bmm(Q,x))+torch.bmm(torch.transpose(p.unsqueeze(2),dim0=2,dim1=1),x)\n",
    "#   t = time.time() - start\n",
    "#   # print(t)\n",
    "#   return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EZRN4j2aEpDH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNFPMbgpdK56sDB9cAVI/Ej",
   "include_colab_link": true,
   "name": "mpc_class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
